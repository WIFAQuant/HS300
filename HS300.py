# %% [markdown]
# # æ²ªæ·±300æŒ‡æ•°çº¯å› å­ç»„åˆæ„å»º
#
# > WIFAé‡åŒ–ç»„ï¼Œ2019å¹´æ˜¥ã€‚
#
# ä¾æ®å¤šå› å­æ¨¡å‹ï¼Œå°è¯•å¯¹æ²ªæ·±300æŒ‡æ•°æ„å»ºçº¯å› å­ç»„åˆã€‚
#
# æ³¨ï¼šç”±äºæ•°æ®éƒ½å·²ç»ä¿å­˜åœ¨æœ¬åœ°ï¼Œ
# æ•…ä»£ç ä¸­æå–åŠä¿å­˜æ•°æ®çš„éƒ¨åˆ†éƒ½æš‚æ—¶è¢«æ³¨é‡Šäº†ã€‚


# %%
from scipy.optimize import minimize
# from WindPy import *
# import WindPy as w                           # for data fetching.
import statsmodels.api as sm                 # for OLS result.
from statsmodels import regression           # for OLS.
import math                                  # math calculation.
import matplotlib.pyplot as plt              # specify "plt".
import seaborn as sns                        # for plotting.
import numpy as np                           # for numerical manipulation.
import pandas as pd                          # for wrapping csv file.
import os                                    # for getting working directory.
path = os.getcwd()
sns.set(style="darkgrid")                    # set seaborn style.
plt.rcParams['font.sans-serif'] = ['SimHei']  # For displaying chinese.
plt.rcParams['axes.unicode_minus'] = False   # For displaying minus sign.

# %%
# # Import Wind Module for getting data.
# w.start()

# %% [markdown]
# Step 1ï¼šå› å­æ•°æ®åº“æ„å»º
#
# å› å­æ•°æ®åˆ†ä¸º**é£æ ¼å› å­**å’Œ**é£é™©å› å­**ã€‚
#
# å…¶ä¸­é£æ ¼å› å­åˆåˆ†ä¸ºå¤§ç±»å› å­å’Œç»†åˆ†ç±»å› å­ï¼Œæœ€ç»ˆé£æ ¼å› å­ä¼šç”±ç»†åˆ†ç±»å› å­åˆæˆã€‚
#
# é£æ ¼å› å­å…±é€‰å–ä»¥ä¸‹7ä¸ªå¤§ç±»ä¸­çš„19ä¸ªå› å­ï¼š
#
# - VALUEï¼šEPS_TTM/Pã€BPS_LR/Pã€CFPS_TTM/Pã€SP_TTM/P
# - GROWTHï¼šNetProfit_SQ_YOYã€Sales_SQ_YOYã€ROE_SQ_YOY
# - PROFITï¼šROE_TTMã€ROA_TTM
# - QUALITYï¼šDebt2Assetã€AssetTurnoverã€InvTurnover
# - MOMENTUMï¼šRet1Mã€Ret3Mã€Ret6M
# - VOLATILITYï¼šRealizedVol_3Mã€RealizedVol_6M
# - LIQUIDITYï¼šTurnover_ave_1Mã€Turnover_ave_3M
#
# é£é™©å› å­é€‰å–ä»¥ä¸‹2ä¸ªå¤§ç±»ä¸­çš„2ä¸ªå› å­ï¼š
#
# - INDUSTRYï¼šä¸­ä¿¡ä¸€çº§è¡Œä¸š
# - SIZEï¼šLn_MarketValue

# %%


def get_factors_list():
    '''
    Return factor list. (str list)

    ä¿å­˜æ‰€éœ€å› å­ï¼ˆä¸‡å¾·ï¼‰æŒ‡æ ‡åã€‚

        ä¿å­˜çš„å­—æ®µåå³ä¸‡å¾·é‡‘èAPI(æ­¤å¤„ä½¿ç”¨WindPy)çš„æŒ‡æ ‡å­—æ®µåã€‚

    å…¶ä¸­"pct_chg_1m",
        "pct_chg_3m",
        "pct_chg_6m",
        "stdevry_3m",
        "stdevry_6m",
        ä¸å¥½ä»wsdä¸­å–ã€‚

    æ‰€ä»¥"pct_chg_1m", 
        "pct_chg_3m",   
        "pct_chg_6m"
        æ˜¯æ ¹æ®pct_chgè®¡ç®—çš„ã€‚

    æ³¢åŠ¨ç‡æå–è¦å¡«å¼€å§‹åŒºé—´å’Œæˆªæ­¢åŒºé—´ã€‚ï¼ˆåŒºé—´ä¸ºè¿‘å‡ ä¸ªæœˆï¼‰
    '''
    return [
        "pe_ttm", "pb_lf", "pcf_ncf_ttm", "ps_ttm",
        "yoyprofit", "yoy_or", "yoyroe", "roe_ttm2",
        "roa_ttm2", "debttoassets", "assetsturn", "invturn",
        "pct_chg_1m", "pct_chg_3m", "pct_chg_6m", "stdevry_3m",
        "stdevry_6m", "tech_turnoverrate20", "tech_turnoverrate60", "val_lnmv"
    ]

# %%


def get_large_factors_list():
    '''
    Return large factors list. (str list)

    ä¿å­˜å¤§ç±»å› å­æŒ‡æ ‡åã€‚
    '''
    return [
        'VALUE', 'GROWTH', 'PROFIT',
        'QUALITY', 'MOMENTUM', 'VOLATILITY',
        'LIQUIDITY', 'INDUSTRY', 'SIZE'
    ]

# %% [markdown]
# ç”±äºæ•°æ®é™åˆ¶å’Œå¹³å°é€‰æ‹©ï¼Œæœ€ç»ˆç¡®å®šçš„å› å­å’Œæœ€åˆé€‰å–çš„å› å­æ¯”è¾ƒå¦‚ä¸‹ï¼š
#
# æœ€åˆé€‰å–å› å­|æœ€ç»ˆç¡®å®šå› å­|å› å­è§£é‡Š
# :--:|:--:|:--:
# EPS_TTM/P|PE_TTM|å¸‚ç›ˆç‡
# BPS_LR/P|PB_LF|æŒ‡å®šæ—¥æœ€æ–°å…¬å‘Šè‚¡ä¸œæƒç›Š
# CFPS_TTM/P|PCF_NCF_TTM|å¸‚ç°ç‡ï¼ˆç°é‡‘å‡€æµé‡ï¼‰
# SP_TTM/P|PS_TTM|å¸‚é”€ç‡
# NetProfit_SQ_YOY|YOYPROFIT|å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡
# Sales_SQ_YOY|YOY_OR|è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡
# ROE_SQ_YOY|YOYROE|å‡€èµ„äº§æ”¶ç›Šç‡åŒæ¯”å¢é•¿ç‡
# ROE_TTM|ROE_TTM2|å‡€èµ„äº§æ”¶ç›Šç‡
# ROA_TTM|ROA_TTM2|æ€»èµ„äº§å‡€åˆ©ç‡
# Debt2Asset|DEBTTOASSETS|èµ„äº§è´Ÿå€ºç‡
# AssetTurnover|ASSETSTURN|æ€»èµ„äº§å‘¨è½¬ç‡
# InvTurnover|INVTURN|å­˜è´§å‘¨è½¬ç‡
# Ret1M|PCT_CHG|æ¶¨è·Œå¹…
# Ret3M|PCT_CHG|æ¶¨è·Œå¹…
# Ret6M|PCT_CHG|æ¶¨è·Œå¹…
# RealizedVol_3M|STDEVRY|3æœˆå¹´åŒ–æ³¢åŠ¨ç‡
# RealizedVol_6M|STDEVRY|6æœˆå¹´åŒ–æ³¢åŠ¨ç‡
# Turnover_ave_1M|TECH_TURNOVERRATE20|20æ—¥å¹³å‡æ¢æ‰‹ç‡
# Turnover_ave_3M|TECH_TURNOVERRATE60|60æ—¥å¹³å‡æ¢æ‰‹ç‡
# ä¸­ä¿¡ä¸€çº§è¡Œä¸šåˆ—è¡¨|INDUSTRY_SW|ç”³ä¸‡è¡Œä¸šåç§°
# Ln_MarketValue|VAL_LNMV|å¯¹æ•°å¸‚å€¼

# %%


def get_hs300_stocks_list():
    '''
    Return hs300 stocks list. (pd.DataFrame)
    '''

    file_path = path + "\\H3 Data\\Raw Data\\hs300.csv"

    # If file already exist, load from disk.
    if os.path.isfile(file_path):
        hs300_data = pd.read_csv(open(
            file_path,
            'r',
            encoding="utf-8"
        ), index_col=[0])

    # If file doesn't exist yet, fetch from WindPy.
    # else:
    #     # Getting the stock list of HS300.
    #     hs300_stocks_list = list(w.wset(
    #         "sectorconstituent",
    #         "date=2019-02-20;windcode=000300.SH",  # base on recent date.
    #         usedf=True
    #     )[1]['wind_code'])

    #     hs300_data = pd.DataFrame(
    #         data=hs300_stocks_list,
    #         columns=["HS300"]
    #     )
    #     # Store to disk.
    #     hs300_data.to_csv(file_path)

    return list(hs300_data["HS300"])

# %%


def factor_data_fetching_and_storing(
    start="2005-01-01",
    end="2019-02-20"
):
    '''
    Parameters:
        start: start date (YYYY-MM-DD). (str)
        end: end date (YYYY-MM-DD). (str)
    Return:
        save raw data to "\\H3 Data\\Raw Data\\" as csv.
    '''
    # # Import data from wind and store it as csv.
    # for factor in get_factors_list():
    #     # Get each factor data from Wind.
    #     factor_data = w.wsd(
    #         get_hs300_stocks_list(),
    #         factor,
    #         start,
    #         end,
    #         "Period=M",
    #         usedf=True  # use pandas dataframe.
    #     )[1]            # the result is a tuple and we only need [1].
    #     # Name the data file by it's factor string.
    #     file_path = path + "\\H3 Data\\Raw Data\\" + factor + ".csv"
    #     factor_data.to_csv(file_path)  # store data.


# %%
# factor_data_fetching_and_storing()

# %%


def sw_industry_data_fetching_and_storing():
    '''
    Return: save SHENWAN industry data to "\\H3 Data\\Raw Data\\" as csv.
    '''
    # industry_sw = w.wsd(
    #     get_hs300_stocks_list(),
    #     "industry_sw",
    #     "2019-02-20",
    #     "2019-02-20",  # set the start and end date as the same.
    #     "industryType=1;Period=M",
    #     usedf=True
    # )[1]
    # file_path = path + "\\H3 Data\\Raw Data\\industry_sw.csv"
    # industry_sw.to_csv(file_path)

# %%
# sw_industry_data_fetching_and_storing()

# %% [markdown]
#
# > ï¼ˆæ³¨ï¼šRet1M, Ret3M, Ret6Mçš†ç”±PCT_CHGåˆæˆï¼›RealizedVol_3M, RealizedVol_6Mçš†ç”±UNDERLYINGHISVOL_90Dä»£æ›¿ã€‚ï¼‰
# >
# > æ•°æ®æ¥æºä¸ºä¸‡å¾·é‡‘èæ•°æ®åº“ï¼Œé€šè¿‡WindPy APIè·å–ã€‚
# >
# > å…¶ä¸­â€œæœ€ç»ˆç¡®å®šå› å­â€åˆ—å³ä¸ºå…¶ä¸‡å¾·æŒ‡æ ‡å­—æ®µåã€‚
# >
# > ï¼ˆæ•°æ®ä¿å­˜åœ¨â€œH3 Dataâ€ ("HS300 Data" çš„ç¼©å†™) æ–‡ä»¶å¤¹ä¸­ï¼Œæ ¼å¼ä¸ºCSVï¼Œç›´æ¥ç”¨å…¨å°å†™çš„ä¸‡å¾·æŒ‡æ ‡åå‘½åã€‚
# > å³ "<ä¸‡å¾·æŒ‡æ ‡å>.csv"ï¼Œå¦‚ "pe_ttm.csv"ï¼‰
# >
# > è·å–çš„åŸå§‹æ•°æ®å‚¨å­˜åœ¨"H3 Data/Raw Data"æ–‡ä»¶å¤¹é‡Œã€‚
#
# æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
#
# è¡Œ/åˆ— | è‚¡ç¥¨ä»£å·ï¼ˆ000001.SZï¼‰
# :--|--:
# äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDï¼‰ | ç›¸åº”å› å­æš´éœ²

# %% [markdown]
# # Step 2ï¼šå› å­æ•°æ®å¤„ç†
# 
# > å¯¹å› å­æ•°æ®è¿›è¡Œå¤„ç†ã€‚

# %%


def get_data(
    factor_name, 
    category="Raw", 
    start_year="2009"
):
    '''
    Parameter:
        factor_name: name of factors in Wind. (str)
        category: which category of data. (str)
            - "Raw"
            - "Processed"
            - "Neutralized"
        start_year: the year when data start. (str) 
    Return:
        forward-filled factor data. (pd.DataFrame)
            index: months. (np.int64)
            columns: stocks code list. (str)
    '''
    data = pd.read_csv(open(
        # Extract raw data.
        path + "\\H3 Data\\" + category + " Data\\" + factor_name + ".csv",
        # read-only mode for data protection.
        'r',
        encoding="utf-8"
    ), index_col=[0])

    # Forward-fill nan to make quarter report fill the month.
    data.fillna(method='ffill', inplace=True)

    if factor_name == "industry_sw":
        pass

    else:
        # Composite-data's date is formated already.
        # There'll be a wired bug if you insist to format again. 
        # All of the date would be "1990-01-01". 
        if (category=="Raw") & (
            factor_name not in [
                "pct_chg_1m", 
                "pct_chg_3m", 
                "pct_chg_6m"
            ]
        ):
            # Make all date format in the same way.
            data.index = pd.to_datetime(data.index).strftime('%Y%m%d')
        data = data.loc[start_year+'0131' : '20190131']
    return data

# %%


def pct_chg_composition():
    '''
    Return: composite and store pct_chg_3m, pct_chg_6m factor data.
    '''
    # Turn percentage format from percent to decimal.
    pct_chg_data = get_data("pct_chg",start_year='2007')/100 + 1

    pct_chg_1m = pct_chg_data - 1
    pct_chg_3m = pct_chg_data.rolling(3).apply(lambda x: np.prod(x)) - 1
    pct_chg_6m = pct_chg_data.rolling(6).apply(lambda x: np.prod(x)) - 1

    for factor_data, factor_name in zip(
        [pct_chg_1m, pct_chg_3m, pct_chg_6m],
        ["pct_chg_1m", "pct_chg_3m", "pct_chg_6m"]
    ):
        factor_data.index = pd.to_datetime(
            factor_data.index
        ).strftime('%Y%m%d')

        factor_data.to_csv(
            path
            + "\\H3 Data\\Raw Data\\"
            + factor_name
            + ".csv"
        )

# %%
# pct_chg_composition()

# %%


def get_values(data):
    '''
    Parameter:
        data: input data. (pd.DataFrame)
    Return:
        a list of all values in data except nan. (list)
    '''
    # Collect all non-nan value into data_list.
    value_list = []
    for i in range(len(data.columns)):
        # is there a way to avoid loop?
        value_list += data.iloc[:, i].dropna().tolist()
    return value_list

# %% [markdown]
# å¦‚å›¾ä¸ºä»»å–9ä¸ªå› å­çš„æ²ªæ·±300çš„æš´éœ²æ•°æ®åœ¨2005~2018å¹´åˆ†å¸ƒç»Ÿè®¡å›¾ã€‚ğŸ‘‡

# %%


def overview(
    source_data_function,
    title
):
    '''
    Parameters:
        source_data_function: 
            the function to get source data for plot. (function)
        title: 
            the title of the plot as well as the file. (str)
    Return: save a 3*3 histogram distribution plot of data.
    '''
    factors_list = get_factors_list()[:9]

    plt.figure(figsize=(10, 10))
    for i, factor in zip(
        range(len(factors_list)),
        factors_list
    ):
        plt.subplot(int("33" + str(i+1)))
        sns.distplot(get_values(
            data=source_data_function(factor)
        ))
        plt.title(factor)

    plt.suptitle(title)
    plt.savefig(path + "\\H3 Plots\\" + title + ".png")

# %%
overview(
    source_data_function=get_data,  
    title="ä¸åŒå› å­åœ¨Aè‚¡çš„å†å²æ•°æ®åˆ†å¸ƒ"
)

# %% [markdown]
# ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºåŸå§‹çš„å› å­æ•°æ®éƒ½å­˜åœ¨æå·®è¿‡å¤§ã€åˆ†å¸ƒéå¸¸ä¸å‡åŒ€çš„ç°è±¡ã€‚
# å¤§å¤šæ•°æ•°æ®é›†ä¸­äºä¸€ä¸ªå€¼é™„è¿‘ï¼Œä½†æ˜¯æ€»ä½“æ¥çœ‹å€¼åŸŸåˆéå¸¸å¹¿ã€‚
# 
# è¿‡å¤§æˆ–è¿‡å°çš„æ•°æ®éƒ½ä¼šå½±å“åˆ°ç»Ÿè®¡åˆ†æçš„ç»“æœï¼Œæ‰€ä»¥éœ€è¦å¯¹æ•°æ®è¿›è¡Œå¤„ç†ã€‚
# 
# ## 2.1 å¡«è¡¥ç¼ºå¤±å€¼
# 
# ç”±äºä¸‡å¾·è¾“å‡ºçš„å½“å­£åº¦è´¢åŠ¡æ•°æ®åªåœ¨æŠ¥å‘ŠæœŸæœ‰æ•°æ®ï¼Œè€Œåœ¨è¯¥å­£åº¦çš„å…¶ä»–æœˆä»½æ²¡æœ‰æ•°æ®ï¼Œæ‰€ä»¥é’ˆå¯¹è¿™ä¸ªç°è±¡é‡‡ç”¨â€œ**å‘å‰å¡«å……**â€æ¥å¡«è¡¥ç¼ºå¤±å€¼ã€‚
# 
# ```Python3
# data.fillna(method = 'ffill', inplace = True)
# ```
# é’ˆå¯¹å‰©ä½™çš„ç¼ºå¤±æ•°æ®ï¼Œæˆ‘ä»¬å°†åœ¨æ•°æ®[æ ‡å‡†åŒ–](##2.3æ ‡å‡†åŒ–)å¤„ç†åç»Ÿä¸€å¡«å……ä¸ºé›¶ã€‚
# 
# ## 2.2 å»æå€¼
# 
# å»æå€¼çš„æ–¹æ³•é‡‡ç”¨è°ƒæ•´å› å­å€¼ä¸­çš„ç¦»ç¾¤å€¼è‡³æŒ‡å®šé˜ˆå€¼çš„ä¸Šä¸‹é™ï¼Œä»è€Œå‡å°**ç¦»ç¾¤å€¼**å’Œ**æå€¼**å¯¹ç»Ÿè®¡çš„åå·®ã€‚
# 
# ç¦»ç¾¤å€¼çš„é˜ˆå€¼ä¸Šä¸‹é™å®šä¹‰çš„æ–¹æ³•ä¸»è¦æœ‰ä¸‰ç§ï¼š
# 
# 1. MADæ³•
# 2. 3Ïƒæ³•
# 3. ç™¾åˆ†ä½æ³•
# 
# ### 2.2.1 MADæ³• (Median Absolute Deviation)
# 
# å–å› å­çš„ä¸­ä½æ•°ï¼ŒåŠ å‡æ¯ä¸ªå› å­ä¸è¯¥ä¸­ä½æ•°çš„ç»å¯¹åå·®å€¼çš„ä¸­ä½æ•°ä¹˜ä¸Šç»™å®šå‚æ•°ï¼ˆæ­¤å¤„ç»è¿‡è°ƒå‚è®¾å®šé»˜è®¤ä¸º100å€ï¼‰å¾—åˆ°ä¸Šä¸‹é˜ˆå€¼ã€‚
# 
# ç»è¿‡MADæ³•å»æå€¼åçš„å› å­æ•°æ®æ¦‚è§ˆå¦‚ä¸‹ï¼š

# %%


def MAD_filter(factor_name, n=60,start_year='2009'):
    '''
    Parameter:
        factor_name: name of factors in Wind. (str)
        n: how many times new median. (int)
    Return:
        filtered data. (pd.DataFrame)
    '''
    data = get_data(factor_name,start_year=start_year)
    values = get_values(data)
    median = np.percentile(
        values,
        50
    )
    new_median = np.percentile(
        get_values(abs(data - median)), 50
    )
    min_range = median - n * new_median
    max_range = median + n * new_median
    return data.clip(min_range, max_range, axis=1)

# %%
overview(
    source_data_function=MAD_filter,
    title="ç»å¯¹å€¼å·®ä¸­ä½æ•°æ³•(MADæ³•)å»æå€¼å"
)

# %% [markdown]
# ### 2.2.2 3Ïƒæ³•
# 
# å–æ‰€æœ‰å› å­æ•°æ®çš„æ ‡å‡†å·®ï¼ˆå³Ïƒï¼‰ï¼Œåç¦»å¹³å‡å€¼ç»™å®šå‚æ•°ï¼ˆæ­¤å¤„é»˜è®¤ä¸ºä¸‰å€ï¼‰æ ‡å‡†å·®å¤„è®¾ä¸ºä¸Šä¸‹é˜ˆå€¼ã€‚
# 
# ç»è¿‡3Ïƒæ³•å»æå€¼åçš„å› å­æ•°æ®æ¦‚è§ˆå¦‚ä¸‹ï¼š

# %%


def three_sigma_filter(factor_name, n=3):
    '''
    Parameter:
        factor_name: name of factors in Wind. (str)
        n: how many sigmas. (int)
    Return:
        filtered data. (pd.DataFrame)
    '''
    data = get_data(factor_name)
    values = get_values(data)
    min_range = np.mean(values) - n * np.std(values)
    max_range = np.mean(values) + n * np.std(values)
    return data.clip(min_range, max_range, axis=1)

# %%
overview(
    source_data_function=three_sigma_filter, 
    title="3Ïƒæ³•å»æå€¼å"
)

# %% [markdown]
# ### 2.2.3 ç™¾åˆ†ä½æ³•
# 
# å–ç»™å®šç™¾åˆ†ä½ä½œä¸ºä¸Šä¸‹é˜ˆå€¼ã€‚ï¼ˆæ­¤å¤„ç»è¿‡è°ƒå‚è®¾å®šä¸ºä¸‹é™1.5%ï¼Œä¸Šé™98.5%åˆ†ä½ç‚¹ï¼‰
# 
# ç»è¿‡ç™¾åˆ†ä½æ³•å»æå€¼åçš„å› å­æ•°æ®æ¦‚è§ˆå¦‚ä¸‹ï¼š

# %%


def percentile_filter(factor_name, min=1.5, max=98.5):
    '''
    Parameters:
        factor_name: name of factors in Wind. (str)
        min: minimum percentage. (float)
        max: maximum percentage. (float)
    Return:
        filtered data. (pd.DataFrame)
    '''
    data = get_data(factor_name)
    values = get_values(data)
    min_range = np.percentile(values, min)
    max_range = np.percentile(values, max)
    return np.clip(data, min_range, max_range)


# %%
overview(
    source_data_function=percentile_filter, 
    title="ç™¾åˆ†ä½æ³•å»æå€¼å"
)

# %% [markdown]
# ### 2.2.4 å»æå€¼ç ”ç©¶ã€‚
# 
# å®é™…ä¸Šï¼Œå³ä½¿ç»è¿‡è°ƒå‚å°½å¯èƒ½åœ°ä½¿ä¸‰ç§ä¸»æµçš„å»æå€¼æ–¹æ³•çš„ç»“æœäº’ç›¸æ¥è¿‘ï¼Œå¹¶ä¸è‡³äºå‡ºç°è¿‡äºé›†ä¸­çš„é˜ˆå€¼ï¼Œä»ç„¶æœ‰å¯èƒ½å‡ºç°éå¸¸æ˜¾è‘—ä¸åŒçš„æ•ˆæœã€‚
# 
# ä»¥æ¯è‚¡ç°é‡‘æµä¸ºä¾‹ï¼Œå°†åŸå§‹æ•°æ®å’Œä¸‰ç§å»æå€¼çš„æ–¹æ³•å¤„ç†åçš„å› å­æ•°æ®æ”¾åœ¨åŒä¸€å¼ å›¾é‡Œï¼Œç”±äºå€¼åŸŸç›¸å·®å¤ªå¤§ï¼Œç”šè‡³æ ¹æœ¬æ— æ³•ä»å›¾ä¸­æ‰¾åˆ°ä¸åŒçš„æ–¹æ³•å¯¹åº”çš„å›¾è¡¨ã€‚ï¼ˆå¦‚ä¸‹å›¾ï¼šåˆ†åˆ«é‡‡ç”¨ä¸‰ç§å»æå€¼æ–¹æ³•å¤„ç†åçš„æ¯è‚¡ç°é‡‘æµæ•°æ®ä¸å…¶åŸå§‹æ•°æ®å›¾ğŸ‘‡ï¼‰

# %%


def huge_deviation_filter_method_comparison(factor_name):
    '''
    Return:
        save a histogram distribution plot of 
        a hugely deviated data for different filter method comparison.
    '''
    plt.figure(figsize=(8, 5))
    sns.distplot(get_values(
        data=get_data(factor_name)
    ), label="Original")
    sns.distplot(get_values(
        data=MAD_filter(factor_name)
    ), label="MAD")
    sns.distplot(get_values(
        data=three_sigma_filter(factor_name)
    ), label="3Ïƒ")
    sns.distplot(get_values(
        data=percentile_filter(factor_name)
    ), label="Percentile")
    plt.legend()
    plt.title("ä¸åŒå»æå€¼æ–¹æ³•çš„æ¯”è¾ƒï¼ˆä»¥è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ä¸ºä¾‹ï¼‰")
    plt.savefig(path + "\\H3 Plots\\Comparison" + factor_name + ".png")


# %%
huge_deviation_filter_method_comparison("yoy_or")

# %% [markdown]
# ç©¶å…¶åŸå› ï¼Œæ˜¯å…¶åŸå§‹æ•°æ®çš„é›†ä¸­åº¦å°±éå¸¸é«˜ï¼Œä»¥è‡³äºä¸åŒæ–¹æ³•å»æå€¼è®¡ç®—å‡ºç›¸å·®ç”šè¿œçš„é˜ˆå€¼ã€‚ï¼ˆå¦‚ä¸‹å›¾ï¼šå…¨éƒ¨Aè‚¡æ ·æœ¬æœŸå†…æ¯è‚¡ç°é‡‘æµçš„å¯†åº¦åˆ†å¸ƒå›¾ğŸ‘‡ï¼‰

# %%


def huge_deviation_original_data():
    '''
    Return:
        save a histogram distribution plot of 
        original data with huge deviation.
    '''
    plt.figure(figsize=(8, 5))
    sns.distplot(get_values(
        data=get_data("yoy_or")
    ), label="Percentile")
    plt.legend()
    plt.title("è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ï¼šåŸå§‹æ•°æ®")
    plt.savefig(path + "\\H3 Plots\\original yoy_or.png")


# %%
huge_deviation_original_data()

# %% [markdown]
# æ‰€ä»¥ç»è¿‡ç™¾åˆ†ä½å»æå€¼åï¼Œå°½ç®¡å€¼åŸŸç¼©å°äº†è¿‘100å€ï¼Œä½†ä»ç„¶éå¸¸é›†ä¸­ã€‚
# 
# å¦å¤–ï¼Œè¿™ç§ç¦»å·®è¿‡å¤§çš„æ•°æ®å»æå€¼çš„æ—¶å€™è¿˜ä¼šå‡ºç°ä¸€ä¸ªé—®é¢˜ï¼šé€ æˆé˜ˆå€¼éƒ¨åˆ†å‡ºç°å¼‚å¸¸é«˜çš„â€œè™šå‡â€æ•°æ®ï¼Œè€Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬ä¸æ„¿æ„çœ‹åˆ°çš„ã€‚ï¼ˆå¦‚ä¸‹å›¾ï¼šæ¯è‚¡ç°é‡‘æµç»è¿‡çº¦æŸæœ€ä¸¥æ ¼çš„ç™¾åˆ†ä½å»æå€¼å¤„ç†åçš„åˆ†å¸ƒå›¾ğŸ‘‡ï¼‰

# %%


def huge_deviation_filtered_data():
    '''
    Return:
        save a histogram distribution plot of 
        percentile-filtered data with huge deviation.
    '''
    plt.figure(figsize=(8, 5))
    sns.distplot(get_values(
        data=percentile_filter("yoy_or")
    ), label="Percentile")
    plt.legend()
    plt.title("è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ï¼šç™¾åˆ†ä½å»æå€¼")
    plt.savefig(path + "\\H3 Plots\\percentile filter yoy_or.png")


# %%
huge_deviation_filtered_data()

# %% [markdown]
# > æ³¨æ„å›¾ä¸­ [-50, 150] å¤„å¼‚å¸¸çš„â€œçªèµ·â€ã€‚
# > 
# > è¿™æ˜¯ç”±äºè¿‡å¤šè¶…å‡ºä¸Šä¸‹é˜ˆå€¼çš„æ•°æ®è¢«è¿«è°ƒæ•´ä¸ºä¸Šä¸‹é˜ˆå€¼ï¼Œå¯¼è‡´é˜ˆå€¼å¤„çš„æ•°æ®åˆ†å¸ƒç‰¹åˆ«å¯†é›†ã€‚
# 
# ä½†åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ˆæ•°æ®åˆ†å¸ƒç›¸å¯¹å‡åŒ€æ—¶ï¼Œæ­¤å¤„ä»¥ROEä¸ºä¾‹ï¼‰ï¼Œå„ç§æ–¹æ³•ä»¥åŠåŸå§‹æ•°æ®ç›¸å·®ä¸å¤§ã€‚ï¼ˆå¦‚ä¸‹å›¾ï¼šèµ„äº§å‘¨è½¬ç‡æ•°æ®çš„åŸå§‹æ•°æ®åŠåˆ†åˆ«ç»è¿‡ä¸‰ç§å»æå€¼æ–¹æ³•å¤„ç†åçš„åˆ†å¸ƒå›¾ğŸ‘‡ï¼‰

# %%


def filter_method_comparison():
    '''
    Return:
        save a histogram distribution plot of
        a normal data for different filter method comparison.
    '''
    plt.figure(figsize=(8, 5))
    sns.distplot(get_values(
        data=get_data("roa_ttm2")
    ), label="Original")
    sns.distplot(get_values(
        data=MAD_filter("roa_ttm2")
    ), label="MAD")
    sns.distplot(get_values(
        data=three_sigma_filter("roa_ttm2")
    ), label="3Ïƒ")
    sns.distplot(get_values(
        data=percentile_filter("roa_ttm2")
    ), label="Percentile")
    plt.legend()
    plt.title("ä¸åŒå»æå€¼æ–¹æ³•çš„æ¯”è¾ƒï¼ˆä»¥æ€»èµ„äº§å‡€åˆ©ç‡ä¸ºä¾‹ï¼‰")
    plt.savefig(path + "\\H3 Plots\\Comparison(roa_ttm2).png")


# %%
filter_method_comparison()

# %% [markdown]
# ç»è¿‡æ¯”è¾ƒç ”ç©¶ï¼Œæˆ‘ä»¬æœ€ç»ˆé€‰å–é˜ˆå€¼é€‰å–ç›¸å¯¹æœ€ä¸ºåˆç†ï¼Œè¾ƒå°‘é˜ˆå€¼å¼‚å¸¸â€œçªèµ·â€ï¼ŒåŒæ—¶ä¿ç•™è¾ƒå®½å€¼åŸŸçš„**å‚æ•°å€¼ä¸º60çš„MADæ³•**è¿›è¡Œå»æå€¼å¤„ç†ã€‚
# 
# ## 2.3 æ ‡å‡†åŒ–
# 
# æ ‡å‡†åŒ–å¤„ç†æ•°æ®çš„ç›®çš„å°±æ˜¯å»é™¤å…¶**é‡çº²**ã€‚
# 
# è¿™æ ·åšå¯ä»¥ä½¿å¾—ï¼š
# 
# - æ•°æ®æ›´åŠ é›†ä¸­
# - ä¸åŒæ•°æ®ä¹‹é—´å¯ä»¥äº’ç›¸æ¯”è¾ƒå’Œè¿›è¡Œå›å½’ç­‰
# 
# ä¸»æµçš„æ ‡å‡†åŒ–çš„æ–¹æ³•æœ‰ä¸¤ç§ï¼š
# 
# æ ‡å‡†åŒ–æ–¹æ³•|åŸç†|ä¼˜ç‚¹|ç¼ºç‚¹
# :--|:--|:--:|:--:
# å¯¹åŸå§‹å› å­å€¼æ ‡å‡†åŒ–|å‡å»å‡å€¼åï¼Œé™¤ä»¥æ ‡å‡†å·®|ä¿ç•™æ›´å¤šä¿¡æ¯|å¯¹æ•°æ®åˆ†å¸ƒæœ‰è¦æ±‚
# å¯¹å› å­æ’åºå€¼æ ‡å‡†åŒ–|å› å­æ’åºå€¼è¿›è¡Œä¸Šè¿°å¤„ç†|é€‚ç”¨æ€§æ›´å¹¿æ³›|éå‚æ•°ç»Ÿè®¡æ³•
# 
# å®ƒä»¬éƒ½èƒ½ä½¿å¾—æ•°æ®çš„ï¼š
# 
# - å‡å€¼ä¸º0
# - æ ‡å‡†å·®ä¸º1
# 
# ç”±äºå·²ç»å¯¹æ•°æ®è¿›è¡Œå»æå€¼å¤„ç†ï¼Œæˆ‘ä»¬æœ€ç»ˆé€‰å–å¯¹åŸå§‹å› å­å€¼è¿›è¡Œæ ‡å‡†åŒ–(z-score)çš„æ–¹æ³•è¿›è¡Œæ ‡å‡†åŒ–ã€‚
# 
# > 2.1ï¼Œ 2.2ï¼Œ 2.3çš„æ•°æ®å¤„ç†éƒ¨åˆ†çš„ï¼š
# >
# > æ•°æ®ä¿å­˜åœ¨"H3 Data/Processed Data"æ–‡ä»¶å¤¹é‡Œã€‚

# %%
# Use z-score method to standardize.


def standardize(factor_name,start_year = '2009'):
    '''
    Parameter:
        factor_name: name of factors in Wind. (str)
        start_year:the start_year the data start
    Return:
        standardized and Filtered (MAD) data. (pd.DataFrame)
    '''
    data = MAD_filter(factor_name,start_year = start_year)
    # data = data.fillna(0)
    mean = np.mean(data)
    std = np.std(data)
    return (data - mean) / std

# %%


def process_and_store_data(start_year):
    '''
    Return:
        save processed data in "\\H3 Data\\Processed Data\\".
        ("processed" means filtered & standardized.)
    '''
    for factor in get_factors_list():
        processed_data = standardize(factor,start_year = start_year)
        file_path = path + "\\H3 Data\\Processed"+start_year+" Data\\" + factor + ".csv"
        processed_data.to_csv(file_path)

# %%
# process_and_store_data('2007')
# process_and_store_data('2009')

# %% [markdown]
# 
# ï¼ˆå¦‚ä¸‹å›¾ä¸ºç»è¿‡å»æå€¼ã€æ ‡å‡†åŒ–å¤„ç†åçš„æ•°æ®å¯†åº¦åˆ†å¸ƒå›¾ä¸€è§ˆğŸ‘‡ï¼‰

# %%


def overview_processed_data():
    # Get an overview of processed data.
    plt.figure(figsize=(10, 10))
    for i in range(9):
        plt.subplot(int("33" + str(i+1)))
        sns.distplot(get_values(
            data=get_data(get_factors_list()[i], category="Processed2009")
        ))
        plt.title(get_factors_list()[i])
    plt.suptitle("ç»è¿‡å¤„ç†åçš„Aè‚¡å› å­æ•°æ®å¯†åº¦åˆ†å¸ƒå›¾ä¸€è§ˆ")
    plt.savefig(path + "\\H3 Plots\\Processed Data.png")


# %%
overview_processed_data()

# %% [markdown]
# ## 2.4 ä¸­æ€§åŒ–
# 
# ä¸­æ€§åŒ–çš„ç›®çš„æ˜¯å‰”é™¤æ•°æ®ä¸­å¤šä½™çš„é£é™©æš´éœ²ã€‚
# 
# æ ¹æ®æŸäº›å› å­ï¼ˆæŒ‡æ ‡ï¼‰é€‰è‚¡çš„æ—¶å€™ï¼Œç”±äºæŸäº›å› å­ä¹‹é—´å…·æœ‰è¾ƒå¼ºçš„ç›¸å…³æ€§ï¼Œæ•…æ—¶å¸¸ä¼šæœ‰æˆ‘ä»¬ä¸å¸Œæœ›çœ‹åˆ°çš„â€œ**åå‘**â€ï¼Œå¯¼è‡´æŠ•èµ„ç»„åˆä¸å¤Ÿ**åˆ†æ•£**ã€‚
# 
# ä¾‹å¦‚ä»¥ä¸‹å››ä¸ªæŒ‡æ ‡ï¼š
# 
# - å¸‚ç°ç‡
# - å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡
# - å‡€èµ„äº§æ”¶ç›Šç‡åŒæ¯”å¢é•¿ç‡
# - å­˜è´§å‘¨è½¬ç‡

# %%


def get_industry_list():
    '''
    Return:
        industry list in HS300 stocks list.
    '''
    return list(get_data("industry_sw").iloc[:, 0].unique())

# %%


def industry_comparison(factor_name):
    '''
    Parameter:
        factor_name: name of factors in Wind. (str)
    Return:
        average factor value of each industries. (pd.DataFrame)
            index: industry. (str)
            columns: factor name. (str)
    '''
    # All industry in HS300.
    # Use certain factor data for comparison example between industry.
    compare_data = get_data(factor_name, start_year='2009')
    compare_industry = pd.DataFrame(
        index=get_industry_list(),
        columns=[factor_name]
    )
    for industry in get_industry_list():
        industry_stock_code_list = list(get_data("industry_sw")[
            get_data("industry_sw").iloc[:, 0] == industry  
        ].index)
        # Some industry is not in HS300.
        try:
            industry_data = compare_data[industry_stock_code_list]
            compare_industry.loc[
                industry, factor_name
            ] = np.mean(np.mean(industry_data))
        except:
            continue
    compare_industry.dropna(inplace=True)
    return compare_industry

# %% [markdown]
# æ²ªæ·±300è‚¡ç¥¨æŒ‡æ•°ä¸­å…±åŒ…å«17ä¸ªè¡Œä¸šï¼ˆæ ¹æ®ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ†ç±»ï¼‰ï¼Œåˆ†åˆ«ç»Ÿè®¡æ²ªæ·±300æŒ‡æ•°ä¸­å„è¡Œä¸šä»¥ä¸Šå››ä¸ªæŒ‡æ ‡çš„å¹³å‡å€¼ï¼Œç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºğŸ‘‡ã€‚

# %%


def plot_industry_comparison():
    '''
    Return:
        save a 2*2 plot of average factor of each industries, 
        which are all siginificantly different. 
    '''
    # Choose 4 factors that's significantly different among industries.
    significant_comparison_industry_list = [
        "pcf_ncf_ttm",
        "yoyprofit",
        "yoyroe",
        "invturn"
    ]
    plt.figure(figsize=(21, 18))  # it's a big plot.
    for i in range(len(significant_comparison_industry_list)):
        plot_data = industry_comparison(
            significant_comparison_industry_list[i]
        )
        plt.subplot(int("22" + str(i+1)))
        sns.barplot(
            x=plot_data.index,
            y=significant_comparison_industry_list[i],
            data=plot_data
        )
        plt.xticks(rotation=60)  # rotate to avoid overlap text.
        plt.title(
            significant_comparison_industry_list[i],
            fontsize=21
        )
    plt.suptitle(
        "æ²ªæ·±300ä¸­ä¸åŒè¡Œä¸šéƒ¨åˆ†å› å­å¹³å‡å€¼æ¯”è¾ƒ",
        fontsize=36
    )
    plt.savefig(path + "\\H3 Plots\\Industry Comparison.png")


# %%
plot_industry_comparison()

# %% [markdown]
# ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œä¸åŒè¡Œä¸šçš„ä¸åŒæŒ‡æ ‡ç›¸å·®åå€ã€åƒå€ä¹ƒè‡³ä¸‡å€éƒ½æœ‰ã€‚
# 
# > *æœ‰è‰²é‡‘å±è¡Œä¸šçš„å¹³å‡å¸‚ç°ç‡æ˜¯é“¶è¡Œä¸šçš„è¿‘è´Ÿå››åä¸‡å€ã€‚*

# %%
print(round(
    industry_comparison("pcf_ncf_ttm").loc["æœ‰è‰²é‡‘å±", "pcf_ncf_ttm"] /
    industry_comparison("pcf_ncf_ttm").loc["å®¶ç”¨ç”µå™¨", "pcf_ncf_ttm"],
    0
))

# %% [markdown]
# é‚£ä¹ˆï¼Œä¾æ®å¸‚ç°ç‡å› å­é€‰å–å‡ºçš„è‚¡ç¥¨å¿…ç„¶å¯¹å¹³å‡å¸‚ç°ç‡é«˜çš„è¡Œä¸šæœ‰åå‘ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›æŠ•èµ„ç»„åˆä¸­çš„è¡Œä¸šå°½å¯èƒ½åˆ†æ•£ï¼Œæ•…æˆ‘ä»¬å¸Œæœ›å¯¹è¡Œä¸šè¿›è¡Œä¸­æ€§åŒ–ã€‚ï¼ˆåŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›å¯¹å¸‚å€¼è¿›è¡Œä¸­æ€§åŒ–ã€‚ï¼‰
# 
# ä¸­æ€§åŒ–çš„ä¸»è¦åšæ³•å°±æ˜¯é€šè¿‡å›å½’å¾—åˆ°ä¸€ä¸ªä¸é£é™©å› å­ï¼ˆè¡Œä¸šå› å­ã€å¸‚å€¼å› å­ï¼‰**çº¿æ€§æ— å…³**çš„å› å­ã€‚ï¼ˆå³çº¿æ€§å›å½’åçš„æ®‹å·®é¡¹ä½œä¸ºä¸­æ€§åŒ–åçš„æ–°å› å­ã€‚ï¼‰å¦‚æ­¤ä¸€æ¥ï¼Œä¸­æ€§åŒ–å¤„ç†åçš„å› å­ä¸é£é™©å› å­ä¹‹é—´çš„ç›¸å…³æ€§å°±ä¸¥æ ¼ä¸ºé›¶ã€‚
# 
# > ä¸è¿‡è¿™æ ·åšä¸­æ€§åŒ–å¹¶ä¸ä¸€å®šæ€»èƒ½å½»åº•åœ°å‰”é™¤å› å­çš„å¤šä½™ä¿¡æ¯ã€‚å› ä¸ºçº¿æ€§å›å½’è¦æ±‚ä¸¤ä¸ªå‰æå‡è®¾ï¼š
# > 
# > - å› å­ä¹‹é—´çº¿æ€§ç›¸å…³
# > - æ®‹å·®æ­£æ€ç‹¬ç«‹åŒåˆ†å¸ƒ
# >
# > è€Œåœ¨å› å­æ•°æ®ä¸­è¿™ä¸¤ä¸ªå‡è®¾éƒ½ä¸ä¸€å®šæˆç«‹ã€‚ï¼ˆä¾‹å¦‚åœ¨[2.2å»æå€¼](##2.2å»æå€¼)æ­¥éª¤ä¸­å¯†åº¦è¿‡é«˜çš„é˜ˆå€¼å°±å¯¹æ•°æ®çš„åˆ†å¸ƒé€ æˆäº†ç ´åï¼‰
# 
# ä½†ç›´è§‚çš„è¯´ï¼Œæ ¹æ®[Brinsonèµ„äº§é…ç½®åˆ†æ](https://www.investopedia.com/terms/a/attribution-analysis.asp)è¶…é¢æ”¶ç›Šç†è®ºæ¥çœ‹ï¼Œå¦‚æœæŠ•èµ„ç»„åˆä¸­é£é™©å› å­é…ç½®èµ„äº§æƒé‡ç­‰äºåŸºå‡†èµ„äº§ä¸­å…¶ä¹‹æƒé‡ï¼Œåˆ™åšåˆ°äº†ä¸­æ€§åŒ–ã€‚
# 
# æ­¤å¤„ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ä¾ç„¶é‡‡ç”¨çº¿æ€§å›å½’ä½œä¸ºä¸­æ€§åŒ–çš„å¤„ç†æ–¹æ³•ã€‚
# 
# å›å½’æ–¹å¼å¦‚ä¸‹ï¼š
# 
# - è¢«è§£é‡Šå˜é‡ï¼šå‰è¿°æ•°æ®å¤„ç†åçš„å› å­æ•°æ®
# 
# - è§£é‡Šå˜é‡ï¼š
# 
#   - å¸‚å€¼å› å­
#   - è¡Œä¸šå› å­ï¼ˆä½œä¸ºæŒ‡ç¤ºå˜é‡ï¼‰
# 
# æœ€ç»ˆå›å½’æ–¹ç¨‹çš„**æ®‹å·®**é¡¹å³ä¸ºä¸­æ€§åŒ–åçš„å› å­æš´éœ²ã€‚
# 
# ï¼ˆå¦‚ä¸‹å›¾ï¼Œä¸ºé€‰å–å››ä¸ªå› å­æŒ‡æ ‡è¿›è¡Œè¡Œä¸šä¸­æ€§åŒ–å‰åçš„ç»“æœï¼Œä»¥å±•ç¤ºä¸­æ€§åŒ–çš„ä¸€èˆ¬ç»“æœğŸ‘‡ï¼Œå¯ä»¥çœ‹å‡ºä¸­æ€§åŒ–å¯¼è‡´åˆ†å¸ƒæ›´å‡åŒ€ã€æ›´æ¥è¿‘å‡å€¼ï¼‰

# %%


def get_industry_exposure(factor_name):
    '''
    Parameter:
        factor_name: name of factors in Wind. (str)
    Return:
        industry exposure data. (pd.DataFrame)
    '''
    file_path = path + "\\H3 Data\\Neutralized2009 Data\\industry exposure " + factor_name + ".csv"
    if os.path.isfile(file_path):
        industry_exposure = pd.read_csv(
            open(
                file_path,
                'r',
                encoding="utf-8"
            ),
            index_col=[0]
        )
    else:
        # Don't know why but different factor data \
        # has different hs300 stocks list,
        # so specify which factor is essential.
        hs300_stock_list = list(get_data(
            factor_name, 
            category="Processed2009",
            start_year='2009'
        ).columns)
        industry_exposure = pd.DataFrame(
            index=get_industry_list(),
            columns=hs300_stock_list
        )
        for stock in hs300_stock_list:
            try:
                industry_exposure.loc[
                    get_data("industry_sw").loc[
                        stock,
                        "INDUSTRY_SW"
                    ],
                    stock
                ] = 1
            except:
                continue
        industry_exposure.fillna(0, inplace=True)
        industry_exposure.to_csv(file_path)
    return industry_exposure

# %%


def neutralize(
    factor_name, 
    start_year="2009", 
    market_capital=True,
    industry=True
):
    '''
    Parameters:
        factor_name: name of factors in Wind. (str)
        market_capital: whether market-capital-neutralize or not. (bool)
        industry: whether industry-neutralize or not. (bool)
    Return:
        neutralized data. (pd.DataFrame)
    '''
    # don't know why but there's still nan.
    y = get_data(
        factor_name, 
        category="Processed"+start_year,
        start_year=start_year
    ).T
    industry_dummy = get_industry_exposure(factor_name).T.fillna(0)
    if market_capital:
        ln_market_capital = get_data(
            "val_lnmv", 
            category="Processed"+start_year,
            start_year=start_year
        ).T
        if industry:
            x = pd.concat([
                    ln_market_capital,
                    industry_dummy
                ], axis=1)
        else:
            x = ln_market_capital
    elif industry:
        x = industry_dummy
    
    x.fillna(0, inplace=True)
    y.fillna(0, inplace=True)
    y = y.loc[list(x.index), :]
    
    result = sm.OLS(y, x).fit()
    
    return result.resid.T

# %%


def plot_industry_neutralization(factor_name):
    '''
    Return: 
        a plot of neutralization comparison.
    '''
    plt.figure(figsize=(8, 5))
    sns.kdeplot(get_values(
        data=get_data(factor_name, category="Processed2009")
    ), label="æœªç»ä¸­æ€§åŒ–")
    sns.kdeplot(get_values(
        data=neutralize(
            factor_name,
            market_capital=False,
            industry=True
        )
    ), label="è¡Œä¸šä¸­æ€§åŒ–")
    plt.legend()
    plt.title("å¯¹" + factor_name + "è¿›è¡Œä¸­æ€§åŒ–å¤„ç†å‰åæ¯”è¾ƒ")
    plt.savefig(path + "\\H3 Plots\\industry neutralization.png")

# %%


def overview_industry_neutralization(factor_list):
    '''
    Parameter:
        factor_list: list of factor names. (list)
    Return:
        save a 2*2 plot of neutralization comparison.
    '''
    plt.figure(figsize=(10, 10))
    for i in range(len(factor_list)):
        plt.subplot(int("22" + str(i+1)))
        sns.kdeplot(get_values(
            data=get_data(factor_list[i], category="Processed2009")
        ), label="æœªç»ä¸­æ€§åŒ–")
        sns.kdeplot(get_values(
            data=neutralize(
                factor_list[i],
                market_capital=False,
                industry=True
            )
        ), label="è¡Œä¸šä¸­æ€§åŒ–")
        plt.legend()
        plt.title("å¯¹" + factor_list[i] + "è¿›è¡Œè¡Œä¸šä¸­æ€§åŒ–å¤„ç†å‰åæ¯”è¾ƒ")
    plt.suptitle("è¡Œä¸šä¸­æ€§åŒ–çš„å…¸å‹ç»“æœ")
    plt.savefig(path + "\\H3 Plots\\overview industry neutralization.png")

# %%
overview_industry_neutralization([
    "pb_lf",
    "debttoassets",
    "assetsturn",
    "invturn"
])

# %% [markdown]
# å¯¹å¸‚å€¼è¿›è¡Œä¸­æ€§åŒ–ä¹Ÿæœ‰ç±»ä¼¼çš„æ•ˆæœã€‚ï¼ˆå¦‚ä¸‹å›¾ä¸ºå¯¹"pb_lf"å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–çš„ç»“æœğŸ‘‡ï¼‰

# %%


def plot_market_neutralization(factor_name):
    '''
    Return: 
        a plot of neutralization comparison.
    '''
    plt.figure(figsize=(8, 5))
    sns.kdeplot(get_values(
        data=get_data(factor_name, category="Processed2009")
    ), label="æœªç»ä¸­æ€§åŒ–")
    sns.kdeplot(get_values(
        data=neutralize(
            factor_name,
            market_capital=True,
            industry=False
        )
    ), label="å¸‚å€¼ä¸­æ€§åŒ–")
    plt.legend()
    plt.title("å¯¹" + factor_name + "è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†å‰åæ¯”è¾ƒ")
    plt.savefig(path + "\\H3 Plots\\market neutralization.png")

# %%
plot_market_neutralization("pb_lf")

# %% [markdown]
# åŒæ ·æ˜¯"pb_lf"å› å­ï¼ŒåŒæ—¶å¯¹å¸‚å€¼å’Œè¡Œä¸šè¿›è¡Œä¸­æ€§åŒ–ğŸ‘‡ï¼Œæ•ˆæœä¹Ÿæ˜¯ç›¸è¿‘çš„ã€‚

# %%


def plot_neutralization(factor_name):
    '''
    Return: 
        a plot of neutralization comparison.
    '''
    plt.figure(figsize=(8, 5))
    sns.kdeplot(get_values(
        data=get_data(factor_name, category="Processed2009")
    ), label="æœªç»ä¸­æ€§åŒ–")
    sns.kdeplot(get_values(
        data=neutralize(
            factor_name,
            market_capital=True,
            industry=True
        )
    ), label="è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–")
    plt.legend()
    plt.title("å¯¹" + factor_name + "è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†å‰åæ¯”è¾ƒ")
    plt.savefig(path + "\\H3 Plots\\industry & market neutralization.png")

# %%
plot_neutralization("pb_lf")

# %% [markdown]
# 
# > æ•°æ®å¤„ç†ä¸­æ€§åŒ–éƒ¨åˆ†çš„ï¼š
# >
# > æ•°æ®ä¿å­˜åœ¨"H3 Data/Neutralized Data"æ–‡ä»¶å¤¹é‡Œã€‚
# 
# æœ€ç»ˆç»è¿‡æ‰€æœ‰å› å­æ•°æ®å¤„ç†æ­¥éª¤ä¹‹åï¼ŒåŸæ¥çš„å› å­æ•°æ®åˆ†å¸ƒå›¾å˜ä¸ºäº†è¿™æ ·ã€‚
# 
# ï¼ˆç»è¿‡æ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤åçš„å› å­æ•°æ®å¯†åº¦åˆ†å¸ƒå›¾ä¸€è§ˆğŸ‘‡ï¼‰

# %%


def neutralize_and_store_data(start_year):
    '''
    Return:
        save industry neutralized data in
        "\\H3 Data\\Neutralized Data\\".
    '''
    for factor in get_factors_list():
        file_path = path + "\\H3 Data\\Neutralized"+start_year+" Data\\" + factor + ".csv"
        neutralized_data = neutralize(
            factor,start_year = start_year,
            market_capital=True,
            industry=True
        )
        neutralized_data.to_csv(file_path)
# %%
# neutralize_and_store_data('2009')
# neutralize_and_store_data('2007')
# %%


def overview_after_data_processing():
    # Get an overview of data after processing.
    plt.figure(figsize=(10, 10))
    for i in range(9):
        plt.subplot(int("33" + str(i+1)))
        factor_name = get_factors_list()[i]
        sns.distplot(get_values(
            data=get_data(factor_name, category="Neutralized2009")
        ))
        plt.title(factor_name)
    plt.suptitle("ç»è¿‡æ•°æ®å¤„ç†åçš„ä¸åŒå› å­åœ¨Aè‚¡çš„å†å²æ•°æ®åˆ†å¸ƒ")
    plt.savefig(path + "\\H3 Plots\\overview after data processing.png")


# %%
overview_after_data_processing()

# %% [markdown]
# Step 3ï¼šå¤§ç±»å› å­åˆæˆ
# 
# å‰é¢ä¸¤ä¸ªæ­¥éª¤å·²ç»æŠŠé£æ ¼å› å­çš„ç»†åˆ†ç±»å› å­æ•°æ®ç»è¿‡æ•°æ®å¤„ç†å¹¶ä¿å­˜äº†ä¸‹æ¥ï¼Œ
# è¿™ä¸€æ­¥æŠŠç»†åˆ†ç±»å› å­åˆæˆä¸ºå¤§ç±»å› å­ã€‚ä½¿å¾—æœ€ç»ˆåˆæˆååªå‰©ä¸‹ï¼š
# 
# - VALUE
# - GROWTH
# - PROFIT
# - QUALITY
# - VOLATILITY
# - MOMENTUM
# - LIQUIDITY
# 
# è¿™ä¸ƒä¸ªå› å­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ„å»ºè¿™ä¸ƒä¸ªå› å­çš„**çº¯å› å­ç»„åˆ**ã€‚
# 
# > ä»è¿™ä¸€æ­¥å¼€å§‹ä¸ºæ–¹ä¾¿æå–æ•°æ®ï¼Œå°†æ•°æ®ä»"pandas.DataFrame"
# è½¬æ¢ä¸º"pandas.PanelData"ã€‚
# >
# > æ•°æ®æ ¼å¼ä¸ºï¼š
# >
# > - index: stock codes
# > - factor names
# >
# > å¯ä»¥ç”¨ä»¥ä¸‹æ–¹æ³•æå–ç‰¹å®šæ—¶é—´çš„æ‰€æœ‰å› å­çš„æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼š
# >
# > ```Python3
# > Large_factor.major_xs("20050131")
# > ```
# 
# å¤§ç±»å› å­åˆæˆçš„æ–¹å¼æ˜¯é€šè¿‡IC_IRåŠ æƒåˆæˆã€‚
# 
# > å¤§ç±»å› å­åˆæˆéƒ¨åˆ†çš„ï¼š
# >
# > æ•°æ®ä¿å­˜åœ¨"H3 Data/Composition Data"æ–‡ä»¶å¤¹é‡Œã€‚

# %%
# Turn dataframe into panel data.


def get_group_data(factor_list, start_year="2009"):
    datadict = {}
    for i in factor_list:
        # This should be the processed data.
        df = get_data(
            i, 
            category="Neutralized"+start_year,
            start_year=start_year
        )  
        datadict[i] = df
    panel = pd.Panel(datadict)
    return panel

# %%


class Large_factor_merge(object):
    def __init__(self, Large_factor):
        if Large_factor == 'VALUE':
            list = ["pe_ttm", "pb_lf", "pcf_ncf_ttm", "ps_ttm"]

        elif Large_factor == 'GROWTH':
            list = ["yoyprofit", "yoy_or", "yoyroe"]

        elif Large_factor == 'PROFIT':
            list = ["roe_ttm2", "roa_ttm2"]

        elif Large_factor == 'QUALITY':
            list = ["debttoassets", "assetsturn", "invturn"]

        elif Large_factor == 'MOMENTUM':
            list = ['pct_chg_1m', 'pct_chg_3m', 'pct_chg_6m']

        elif Large_factor == 'VOLATILITY':
            list = ["stdevry_3m", "stdevry_6m"]

        elif Large_factor == 'LIQUIDITY':
            list = ["tech_turnoverrate60", "tech_turnoverrate20"]

        self.data = get_group_data(list, "2007")
        self.data_2009 = get_group_data(list, "2009")
        self.Large_factor = Large_factor
    # Define the following function for you can read clearly 
    # and can acquire the data of every step.

    def Caculate_IC(self):
        stock_return = get_data(
            "pct_chg_1m", 
            category="raw",
            start_year="2007"
        )
        datadict = {}
        for i in self.data.items:
            df = self.data[i]
            IC = pd.DataFrame(columns=['IC_monthly'],
                              index=df.index[1:len(df)])
            IC_group = []
            for j in range(len(df)-1):
                cor = df.iloc[j].corr(stock_return.iloc[j+1])
                IC_group.append(cor)
            IC['IC_monthly'] = IC_group
            datadict[i] = IC
        IC_Large = pd.Panel(datadict)
        return IC_Large

    def Factors_merge_Static(self):
        IC_Large = self.Caculate_IC()
        weight_df = pd.DataFrame(
            columns=['weights'], 
            index=self.data.items
        )
        weight = []
        for i in IC_Large.items:
            df = IC_Large[i]
            IR = df.iloc[-24:, 0].mean()/df.iloc[-24:, 0].std()
            weight.append(IR)
        #weight = [x / sum(weight) for x in weight]  # adjust the sum of weight to 1.0
        weight_df['weights'] = weight
        weight = weight_df
        Factors_sum = pd.DataFrame(
            0, 
            columns=self.data_2009.minor_axis, 
            index=self.data_2009.major_axis
        )
        for i in self.data.items:
            df = self.data_2009[i]
            new_df = df * weight.loc[i, 'weights']
            Factors_sum = Factors_sum + new_df
        return Factors_sum

    def Factors_merge_dynamic(self):
        IC_Large = self.Caculate_IC()
        weight_df = pd.DataFrame(columns=IC_Large.major_axis[23:], index=IC_Large.items)
        for i in IC_Large.items:
            for j in range(23, len(IC_Large.major_axis)):
                df = IC_Large[i]
                IR = df.iloc[j - 23:j+1, 0].mean() / df.iloc[j - 23:j+1, 0].std()
                weight_df.loc[i, IC_Large.major_axis[j]] = IR
        #weight_df = weight_df.apply(lambda x: x / sum(x))
        weight = weight_df
        Factors_sum = pd.DataFrame(0, columns=self.data.minor_axis, index=weight.columns)
        for i in self.data_2009.items:
            df = self.data_2009[i]
            new_df = df.mul(weight.loc[i], axis=0)
            Factors_sum = Factors_sum + new_df
        return Factors_sum

# %%


def Merge_and_store_factors_Dynamic():
    Factor_dict = {}
    for i in ['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY']:
        Factor_data = Large_factor_merge(i).Factors_merge_dynamic()
        Factor_dict[i] = Factor_data
        file_path = path + "\\H3 Data\\Large Factor Dynamic Data\\" + i + ".csv"
        Factor_data.to_csv(file_path)
    Large_factor = pd.Panel(Factor_dict)
    return Large_factor
Large_factor_dynamic = Merge_and_store_factors_Dynamic()
# when you want to use one factor,you can edit'Large_factor[the name of the factor]'
# %%


def Merge_and_store_factors_Static():
    Factor_dict = {}
    for i in ['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY']:
        Factor_data = Large_factor_merge(i).Factors_merge_Static()
        Factor_dict[i] = Factor_data
        file_path = path + "\\H3 Data\\Large Factor Static Data\\" + i + ".csv"
        Factor_data.to_csv(file_path)
    Large_factor = pd.Panel(Factor_dict)
    return Large_factor
Large_factor_Static = Merge_and_store_factors_Static()

# %%

def get_Large_Factors(factor_name, type):
    category = "Large Factor " + type
    data = get_data(factor_name, category=category)
    return data

#%%


def overview_Large_factors(type):
    # Get an overview of data after processing.
    plt.figure(figsize = (10, 10))
    for i in range(7):
        plt.subplot(int("33" + str(i+1)))
        factor_name = get_large_factors_list()[i]
        sns.distplot(get_values(
            data = get_Large_Factors(factor_name,type)
        ))
        plt.title(factor_name)
    plt.suptitle("å¤§ç±»å› å­åœ¨Aè‚¡çš„å†å²æ•°æ®åˆ†å¸ƒ(" + type + ' synthesis)')
    plt.savefig(path + "\\H3 Plots\\Large Factors "+ type +".png")

# %% [markdown]
# åˆæˆåçš„å¤§ç±»å› å­æ•°æ®å¦‚ä¸‹å›¾ï¼ŒåŠ¨æ€æƒé‡åˆæˆğŸ‘‡ã€‚

# %%
overview_Large_factors('dynamic')

# %% [markdown]
# é™æ€æƒé‡åˆæˆğŸ‘‡ã€‚

# %%
overview_Large_factors('Static')

# %% [markdown]
from sklearn import metrics

# %% å¤šå…ƒçº¿æ€§å›å½’

def get_regression_data(time, next_time, type):
    # get 7(å¤§ç±»)+1ï¼ˆmarketï¼‰+28ï¼ˆè¡Œä¸šï¼‰+1ï¼ˆreturnï¼‰ data list for one stock
    data = pd.DataFrame(
        columns=[
            'return', 'VALUE', 'GROWTH', 'PROFIT', 
            'QUALITY', 'MOMENTUM', 'VOLATILITY', 'LIQUIDITY',
            'market'
        ]
    )
    
    for factor_name in [
        'VALUE', 'GROWTH', 'PROFIT', 
        'QUALITY', 'MOMENTUM', 'VOLATILITY', 'LIQUIDITY'
    ]:
        data[factor_name] = get_Large_Factors(factor_name, type).loc[time]
        
    data['market'] = get_data("val_lnmv", category="Processed", start_year="2009").T[time]
    data['return'] = get_data("pct_chg_1m", category="raw").loc[next_time]    
    
    industry_factor = get_industry_exposure('pb_lf').T    
    data = data.join(industry_factor)
    
    return data


def regression_model(y, X):
    model = sm.WLS(y, X)  # åŠ æƒæœ€å°äºŒä¹˜æ³• è§£å†³å¼‚æ–¹å·®æ€§
    results = model.fit()
    # results.summary
    return {'beta': results.params, 'R^2': results.rsquared}


# %% è¿›è¡Œå›å½’ è·å¾—å› å­æ”¶ç›ŠçŸ©é˜µå’ŒR^2
#
def run_regression(type):
    time_list = get_Large_Factors('VALUE', type).dropna(axis=0,how='any'). index
    param_df = pd.DataFrame(columns = time_list[1:])
    R2_temp = []

    for i in range(0,len(time_list)-1):
        regression_data = get_regression_data(time_list[i],time_list[i+1],type)
        regression_data = regression_data.dropna(axis=0,how='any') 
        y = regression_data['return']
        X = regression_data.iloc[:,1:]
        param = regression_model(y,X).get('beta')
        R2_temp = R2_temp + [regression_model(y,X).get('R^2')]
        param_df[time_list[i+1]] = param
        
    R2_list = pd.Series(R2_temp,index = time_list[1:])
    return {'factor_income': param_df, 'R^2': R2_list}

# run_regression('Static')==>è¿›è¡Œå›å½’
    

# %% 
# æ–¹æ³•ä¸€ï¼šä¼°è®¡å› å­é¢„æœŸæ”¶ç›Šï¼Œæ­¤å¤„é‡‡ç”¨N=12çš„å†å²å‡å€¼æ³•

def estimated_factor_expected_income(type):
    F = run_regression(type).get('factor_income').T
    N = 12
    time_list = get_Large_Factors('VALUE', type).dropna(axis=0,how='any'). index
    F_predict = pd.DataFrame(columns=F.columns)
    #for i in range(N, len(time_list)):
    #    F_predict.loc[time_list[i]] = list(F.iloc[i-N:i].mean())
    F_predict.loc[time_list[len(time_list)-1]] = list(F.iloc[len(time_list)-1-N:len(time_list)].mean())
    return F_predict #å› ä¸ºæ— æ³•è·å–æœªæ¥æ—¶é—´æ—¥æœŸå€¼ï¼Œæ­¤å¤„è¿”å›çš„å€¼ä¸ºindexå€¼çš„ä¸‹ä¸€ä¸ªæœˆçš„é¢„æœŸå€¼
 
# æ–¹æ³•äºŒï¼šARIMA
def estimated_factor_expected_income_ARIMA(type):
    F = run_regression(type).get('factor_income').T
#%% æ”¶ç›Šé¢„æµ‹æ¨¡å‹
    
def load_of_factor(time, type): # è·å¾—å› å­è½½è·çŸ©é˜µ 
    data = pd.DataFrame(
        columns=[
             'VALUE', 'GROWTH', 'PROFIT', 
            'QUALITY', 'MOMENTUM', 'VOLATILITY', 'LIQUIDITY',
            'market'
        ]
    )
    
    for factor_name in [
        'VALUE', 'GROWTH', 'PROFIT', 
        'QUALITY', 'MOMENTUM', 'VOLATILITY', 'LIQUIDITY'
    ]:
        data[factor_name] = get_Large_Factors(factor_name, type).loc[time]
        
    data['market'] = get_data("val_lnmv", category="Processed", start_year="2009").T[time] 
    
    industry_factor = get_industry_exposure('pb_lf').T    
    data = data.join(industry_factor)
            
    return data

def calculate_expected_return(type):
    f_predict = estimated_factor_expected_income(type)
    time_list = get_Large_Factors('VALUE', type).dropna(axis=0,how='any'). index
    X = load_of_factor(time_list[-1], type)
    r_predict = X.mul(f_predict,axis=1).T.sum()
    return r_predict
    
def evaluate_model(type):
    # 20190228çš„è‚¡ç¥¨æ”¶ç›ŠçœŸå®å€¼
    real_return_data = pd.read_csv(open(path + "\\H3 Data\\Raw Data\\20190228pct_chg_1m.csv",'r',encoding="utf-8"), index_col=[0])
    predict_return_data = calculate_expected_return(type).to_frame()
    #stock codeé¡ºåºä¸åŒ æ•…åˆå¹¶
    data = real_return_data.join(predict_return_data)   
    ## ç”»å›¾
    plt.plot(data)
    plt.plot(data[0]/data['return'])
    MSE = metrics.mean_squared_error(data['return'],data[0])
    RMSE = np.sqrt(MSE)
    return {'MSE': MSE, 'RMSE': RMSE}

# evaluate_model('Static')
# %% [markdown]
# # STEP 5

# %%
# å‡è®¾äº†ä¹‹å‰å¾ˆå¤šå›å½’çš„è®¡ç®—ç»“æœ
from scipy.optimize import minimize
Factor_income =pd.DataFrame(-1+2*np.random.random((121,9)),columns=['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY','INDUSTRY','SIZE'],
                            index = get_data('ps_ttm', category="Neutralized").index)
Stock_predict = pd.DataFrame(-0.1+np.random.random((300,1))/3,columns=['yeild_forecast'],index = get_data('ps_ttm', category="Neutralized").columns)
Factor_predict = pd.DataFrame(-0.1+np.random.random((300,9))/3,columns=['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY','INDUSTRY','SIZE'],index = get_data('ps_ttm', category="Neutralized").columns)
#æ¯åªè‚¡ç¥¨çš„åœ¨ä¸åŒæ—¶é—´ç‚¹çš„æ®‹å·®ï¼Œå¯ä»¥ç­‰äºå®é™…çš„è‚¡ç¥¨æ”¶ç›Šç‡-é¢„æµ‹çš„è‚¡ç¥¨æ”¶ç›Šç‡
Stock_Residual = pd.DataFrame(-0.1+np.random.random((121,300))/5,columns = get_data('ps_ttm', category="Neutralized").columns,index = get_data('ps_ttm', category="Neutralized").index)


class Portfolio_Optimization(object):
    def __init__(self, Target_factors ,time_window,type):
        time_list = get_Large_Factors('VALUE', type).index
        self.Target_factors = Target_factors
        self.time_window = time_window
        self.type  = type
        self.Factor_income = estimated_factor_expected_income(self.type)
        self.Stock_predict = calculate_expected_return(self.type)
        self.Factor_predict = load_of_factor(time_list[-1], type)

    def Factor_covariance(self):
        factors = self.Factor_income.iloc[-self.time_window:]
        Cov = np.cov(factors.values.T)
        return Cov

    # é¢„æµ‹æ®‹å·®é£é™©è¿™ä¸€éƒ¨åˆ†å¾ˆå¤æ‚ï¼Œç”¨åˆ°åŠè¡°æœŸæƒé‡å’Œè´å¶æ–¯æ”¶ç¼©ï¼Œå’Œæ³¢åŠ¨æ€§è°ƒæ•´ï¼Œå…¶å®ä¸çº¦æŸé£é™©æ—¶ï¼Œä¸ç”¨è®¡ç®—æ­¤é¡¹ï¼Œå…ˆæŠŠæ¡†æ¶æ­èµ·æ¥ï¼Œ
    # åé¢è®¡ç®—ç»„åˆçš„å¤æ™®æ¯”ç‡è¦ç”¨åˆ°ç»„åˆæ–¹å·®ï¼Œå°±è¦ç”¨åˆ°æ®‹å·®é£é™©ï¼Œåç»­ç ”ç©¶æˆ‘ä»¬å†ä»”ç»†ç ”ç©¶è¿™ä¸€éƒ¨åˆ†å…·ä½“æ€ä¹ˆç®—
    def Trait_risk_forecast(self):
        Res = pd.DataFrame(-0.1+np.random.random((300,120))/5)
        return Res

    def optimization(self):
        Cov = self.Factor_covariance()
        Res = self.Trait_risk_forecast()
        yeild_T_1 = self.Stock_predict

        #éçº¿æ€§è§„åˆ’
        x0 = np.random.rand(300)
        x0 /= sum(x0)
        Non_target_factors = list(set(Large_Factors_list) ^ set(self.Target_factors))
        n = len(Non_target_factors)
        m = list(range(n))
        b = [0]*9
        b[0:n-1] = m
        # æ­¤å¤„æˆ‘å°è¯•äº†å„ç§æ–¹æ³•ï¼Œç”¨iéå†éç›®æ ‡çº¯å› å­ç„¶åç”Ÿæˆæ¡ä»¶ï¼Œä½†æ˜¯ç”Ÿæˆçš„æ¡ä»¶åœ¨ä¼˜åŒ–æ¨¡å‹ä¸­æ²¡æœ‰è¢«æˆåŠŸçº¦æŸï¼Œæœ€ååªèƒ½å…¨éƒ¨å†™å‡ºæ¥9ä¸ªå› å­æ¡ä»¶ï¼Œ
        # å¯¹äºç›®æ ‡çº¯å› å­ï¼Œåºæ•°å–çš„0ï¼Œå³æ¡ä»¶æ˜¯é‡å¤çš„éç›®æ ‡å› å­çº¦æŸï¼Œè¿™é‡Œçš„æ¡ä»¶æ•°æ®ç±»å‹æ˜¯tupleï¼Œtupleä¸èƒ½è¢«å¢åŠ ï¼Œæˆ‘è¯•è¿‡å…ˆç”¨listæ·»åŠ ç„¶åè½¬ä¸ºtuple,
        # ä»ç„¶æ²¡æœ‰è¢«æˆåŠŸè¯†åˆ«ï¼Œå¦‚æœæœ‰æ›´ç®€ä¾¿çš„æ–¹æ³•ï¼Œæ¬¢è¿æå‡º
        # æœ€å°åŒ–çš„å‡½æ•°
        func = lambda x: -(yeild_T_1 * np.mat(x).T).sum()[0]
        cons4 = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},
                 {'type': 'ineq','fun': lambda x:(0.03-abs((self.Factor_predict[[Non_target_factors[b[0]]]]*np.mat(x).T).sum()[0]))},
                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[1]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[2]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[3]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[4]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[5]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[6]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[7]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((self.Factor_predict[[Non_target_factors[b[8]]]] * np.mat(x).T).sum()[0]))},
                 )
        # å¦‚æœè¦æ·»åŠ æ³¢åŠ¨æ€§çº¦æŸï¼Œæ¡ä»¶è¦æ”¹ä¸ºä»¥ä¸‹ï¼Œæˆ‘å†™çš„æ˜¯é™åˆ¶æ³¢åŠ¨å°äº3%
        '''cons4 = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},
                 {'type': 'ineq','fun': lambda x: (0.03 -((np.mat(Factor_predict).T*np.mat(x).T).T*Cov*(np.mat(Factor_predict).T*np.mat(x).T))[0,0]
                                    +(np.mat(x)*np.mat(Res)*np.mat(x).T))[0,0]},
                {'type': 'ineq', 'fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[0]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[1]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[2]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[3]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[4]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[5]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[6]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[7]]]] * np.mat(x).T).sum()[0]))},
                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[8]]]] * np.mat(x).T).sum()[0]))},
                 )'''
        c = (0,1)
        bnds = tuple([c]*300)#è¾¹ç•Œæ¡ä»¶ä¸º0-1
        res = minimize(func, x0, method='SLSQP', constraints=cons4,bounds = bnds)
        Stock_weight = pd.DataFrame(res.x,columns=['Portfolio Weight'],index = Stock_predict.index)
        return  Stock_weight, -res.fun

# ç›®æ ‡çº¯å› å­ä¸º'VALUE','GROWTH','PROFIT'ï¼Œä½¿ç”¨å†å²æ—¶é—´æ®µä¸ºè¿‡å»32ä¸ªæœˆï¼Œä»…å¯¹éç›®æ ‡çº¯å› å­åç¦»åšçº¦æŸæ¡ä»¶ï¼Œæœ€å¤§åŒ–æ”¶ç›Šï¼Œè¿”å›æƒé‡å’Œç»„åˆæ”¶ç›Š
# ç”Ÿæˆç»“æœå¹¶è¾“å‡ºæƒé‡å’Œç»„åˆæ”¶ç›Š
def get_optimization_result(Target_factors ,time_window,type):
    [Stock_weight, Portfolio_Return] = Portfolio_Optimization(Target_factors ,time_window,type).optimization()
    Stock_weight['Portfolio Return'] = np.nan
    Stock_weight.iloc[0,-1] = Portfolio_Return
    Stock_weight.to_csv(path + "\\H3 Data\\Portfolio result\\ "+str(Target_factors)+"çº¯å› å­("+str(type)+").csv")