{"cells":[{"cell_type":"markdown","source":[" # Ê≤™Ê∑±300ÊåáÊï∞Á∫ØÂõ†Â≠êÁªÑÂêàÊûÑÂª∫\n","\n"," > WIFAÈáèÂåñÁªÑÔºå2019Âπ¥Êò•„ÄÇ\n","\n"," ‰æùÊçÆÂ§öÂõ†Â≠êÊ®°ÂûãÔºåÂ∞ùËØïÂØπÊ≤™Ê∑±300ÊåáÊï∞ÊûÑÂª∫Á∫ØÂõ†Â≠êÁªÑÂêà„ÄÇ\n","\n"," Ê≥®ÔºöÁî±‰∫éÊï∞ÊçÆÈÉΩÂ∑≤Áªè‰øùÂ≠òÂú®Êú¨Âú∞Ôºå\n"," ÊïÖ‰ª£Á†Å‰∏≠ÊèêÂèñÂèä‰øùÂ≠òÊï∞ÊçÆÁöÑÈÉ®ÂàÜÈÉΩÊöÇÊó∂Ë¢´Ê≥®Èáä‰∫Ü„ÄÇ"],"metadata":{}},{"source":["from scipy.optimize import minimize\n","# from WindPy import *\n","# import WindPy as w                           # for data fetching.\n","import statsmodels.api as sm                 # for OLS result.\n","from statsmodels import regression           # for OLS.\n","import math                                  # math calculation.\n","import matplotlib.pyplot as plt              # specify \"plt\".\n","import seaborn as sns                        # for plotting.\n","import numpy as np                           # for numerical manipulation.\n","import pandas as pd                          # for wrapping csv file.\n","import os                                    # for getting working directory.\n","path = os.getcwd()                           # current working directory.\n","sns.set(style=\"darkgrid\")                    # set seaborn style.\n","plt.rcParams['font.sans-serif'] = ['SimHei']  # For displaying chinese.\n","plt.rcParams['axes.unicode_minus'] = False   # For displaying minus sign.\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# # Import Wind Module for getting data.\n","# w.start()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Step 1ÔºöÂõ†Â≠êÊï∞ÊçÆÂ∫ìÊûÑÂª∫\n","\n"," Âõ†Â≠êÊï∞ÊçÆÂàÜ‰∏∫**È£éÊ†ºÂõ†Â≠ê**Âíå**È£éÈô©Âõ†Â≠ê**„ÄÇ\n","\n"," ÂÖ∂‰∏≠È£éÊ†ºÂõ†Â≠êÂèàÂàÜ‰∏∫Â§ßÁ±ªÂõ†Â≠êÂíåÁªÜÂàÜÁ±ªÂõ†Â≠êÔºåÊúÄÁªàÈ£éÊ†ºÂõ†Â≠ê‰ºöÁî±ÁªÜÂàÜÁ±ªÂõ†Â≠êÂêàÊàê„ÄÇ\n","\n"," È£éÊ†ºÂõ†Â≠êÂÖ±ÈÄâÂèñ‰ª•‰∏ã7‰∏™Â§ßÁ±ª‰∏≠ÁöÑ19‰∏™Âõ†Â≠êÔºö\n","\n"," - VALUEÔºöEPS_TTM/P„ÄÅBPS_LR/P„ÄÅCFPS_TTM/P„ÄÅSP_TTM/P\n"," - GROWTHÔºöNetProfit_SQ_YOY„ÄÅSales_SQ_YOY„ÄÅROE_SQ_YOY\n"," - PROFITÔºöROE_TTM„ÄÅROA_TTM\n"," - QUALITYÔºöDebt2Asset„ÄÅAssetTurnover„ÄÅInvTurnover\n"," - MOMENTUMÔºöRet1M„ÄÅRet3M„ÄÅRet6M\n"," - VOLATILITYÔºöRealizedVol_3M„ÄÅRealizedVol_6M\n"," - LIQUIDITYÔºöTurnover_ave_1M„ÄÅTurnover_ave_3M\n","\n"," È£éÈô©Âõ†Â≠êÈÄâÂèñ‰ª•‰∏ã2‰∏™Â§ßÁ±ª‰∏≠ÁöÑ2‰∏™Âõ†Â≠êÔºö\n","\n"," - INDUSTRYÔºö‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏ö\n"," - SIZEÔºöLn_MarketValue"],"metadata":{}},{"source":["\n","\n","def get_factors_list():\n","    '''\n","    Return factor list. (str list)\n","\n","    ‰øùÂ≠òÊâÄÈúÄÂõ†Â≠êÔºà‰∏áÂæ∑ÔºâÊåáÊ†áÂêç„ÄÇ\n","\n","        ‰øùÂ≠òÁöÑÂ≠óÊÆµÂêçÂç≥‰∏áÂæ∑ÈáëËûçAPI(Ê≠§Â§Ñ‰ΩøÁî®WindPy)ÁöÑÊåáÊ†áÂ≠óÊÆµÂêç„ÄÇ\n","\n","    ÂÖ∂‰∏≠\"pct_chg_1m\",\n","        \"pct_chg_3m\",\n","        \"pct_chg_6m\",\n","        \"stdevry_3m\",\n","        \"stdevry_6m\",\n","        ‰∏çÂ•Ω‰ªéwsd‰∏≠Âèñ„ÄÇ\n","\n","    ÊâÄ‰ª•\"pct_chg_1m\", \n","        \"pct_chg_3m\",   \n","        \"pct_chg_6m\"\n","        ÊòØÊ†πÊçÆpct_chgËÆ°ÁÆóÁöÑ„ÄÇ\n","\n","    Ê≥¢Âä®ÁéáÊèêÂèñË¶ÅÂ°´ÂºÄÂßãÂå∫Èó¥ÂíåÊà™Ê≠¢Âå∫Èó¥„ÄÇÔºàÂå∫Èó¥‰∏∫ËøëÂá†‰∏™ÊúàÔºâ\n","    '''\n","    return [\n","        \"pe_ttm\", \"pb_lf\", \"pcf_ncf_ttm\", \"ps_ttm\",\n","        \"yoyprofit\", \"yoy_or\", \"yoyroe\", \"roe_ttm2\",\n","        \"roa_ttm2\", \"debttoassets\", \"assetsturn\", \"invturn\",\n","        \"pct_chg_1m\", \"pct_chg_3m\", \"pct_chg_6m\", \"stdevry_3m\",\n","        \"stdevry_6m\", \"tech_turnoverrate20\", \"tech_turnoverrate60\", \"val_lnmv\"\n","    ]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def get_large_factors_list():\n","    '''\n","    Return large factors list. (str list)\n","\n","    ‰øùÂ≠òÂ§ßÁ±ªÂõ†Â≠êÊåáÊ†áÂêç„ÄÇ\n","    '''\n","    return [\n","        'VALUE', 'GROWTH', 'PROFIT',\n","        'QUALITY', 'MOMENTUM', 'VOLATILITY',\n","        'LIQUIDITY', 'INDUSTRY', 'SIZE'\n","    ]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Áî±‰∫éÊï∞ÊçÆÈôêÂà∂ÂíåÂπ≥Âè∞ÈÄâÊã©ÔºåÊúÄÁªàÁ°ÆÂÆöÁöÑÂõ†Â≠êÂíåÊúÄÂàùÈÄâÂèñÁöÑÂõ†Â≠êÊØîËæÉÂ¶Ç‰∏ãÔºö\n","\n"," ÊúÄÂàùÈÄâÂèñÂõ†Â≠ê|ÊúÄÁªàÁ°ÆÂÆöÂõ†Â≠ê|Âõ†Â≠êËß£Èáä\n"," :--:|:--:|:--:\n"," EPS_TTM/P|PE_TTM|Â∏ÇÁõàÁéá\n"," BPS_LR/P|PB_LF|ÊåáÂÆöÊó•ÊúÄÊñ∞ÂÖ¨ÂëäËÇ°‰∏úÊùÉÁõä\n"," CFPS_TTM/P|PCF_NCF_TTM|Â∏ÇÁé∞ÁéáÔºàÁé∞ÈáëÂáÄÊµÅÈáèÔºâ\n"," SP_TTM/P|PS_TTM|Â∏ÇÈîÄÁéá\n"," NetProfit_SQ_YOY|YOYPROFIT|ÂáÄÂà©Ê∂¶ÂêåÊØîÂ¢ûÈïøÁéá\n"," Sales_SQ_YOY|YOY_OR|Ëê•‰∏öÊî∂ÂÖ•ÂêåÊØîÂ¢ûÈïøÁéá\n"," ROE_SQ_YOY|YOYROE|ÂáÄËµÑ‰∫ßÊî∂ÁõäÁéáÂêåÊØîÂ¢ûÈïøÁéá\n"," ROE_TTM|ROE_TTM2|ÂáÄËµÑ‰∫ßÊî∂ÁõäÁéá\n"," ROA_TTM|ROA_TTM2|ÊÄªËµÑ‰∫ßÂáÄÂà©Áéá\n"," Debt2Asset|DEBTTOASSETS|ËµÑ‰∫ßË¥üÂÄ∫Áéá\n"," AssetTurnover|ASSETSTURN|ÊÄªËµÑ‰∫ßÂë®ËΩ¨Áéá\n"," InvTurnover|INVTURN|Â≠òË¥ßÂë®ËΩ¨Áéá\n"," Ret1M|PCT_CHG|Ê∂®Ë∑åÂπÖ\n"," Ret3M|PCT_CHG|Ê∂®Ë∑åÂπÖ\n"," Ret6M|PCT_CHG|Ê∂®Ë∑åÂπÖ\n"," RealizedVol_3M|STDEVRY|3ÊúàÂπ¥ÂåñÊ≥¢Âä®Áéá\n"," RealizedVol_6M|STDEVRY|6ÊúàÂπ¥ÂåñÊ≥¢Âä®Áéá\n"," Turnover_ave_1M|TECH_TURNOVERRATE20|20Êó•Âπ≥ÂùáÊç¢ÊâãÁéá\n"," Turnover_ave_3M|TECH_TURNOVERRATE60|60Êó•Âπ≥ÂùáÊç¢ÊâãÁéá\n"," ‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÂàóË°®|INDUSTRY_SW|Áî≥‰∏áË°å‰∏öÂêçÁß∞\n"," Ln_MarketValue|VAL_LNMV|ÂØπÊï∞Â∏ÇÂÄº"],"metadata":{}},{"source":["\n","\n","def get_hs300_stocks_list():\n","    '''\n","    Return hs300 stocks list. (pd.DataFrame)\n","    '''\n","\n","    file_path = path + \"\\\\H3 Data\\\\Raw Data\\\\hs300.csv\"\n","\n","    # If file already exist, load from disk.\n","    if os.path.isfile(file_path):\n","        hs300_data = pd.read_csv(open(\n","            file_path,\n","            'r',\n","            encoding=\"utf-8\"\n","        ), index_col=[0])\n","\n","    # If file doesn't exist yet, fetch from WindPy.\n","    # else:\n","    #     # Getting the stock list of HS300.\n","    #     hs300_stocks_list = list(w.wset(\n","    #         \"sectorconstituent\",\n","    #         \"date=2019-02-20;windcode=000300.SH\",  # base on recent date.\n","    #         usedf=True\n","    #     )[1]['wind_code'])\n","\n","    #     hs300_data = pd.DataFrame(\n","    #         data=hs300_stocks_list,\n","    #         columns=[\"HS300\"]\n","    #     )\n","    #     # Store to disk.\n","    #     hs300_data.to_csv(file_path)\n","\n","    return list(hs300_data[\"HS300\"])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def factor_data_fetching_and_storing(\n","    start=\"2005-01-01\",\n","    end=\"2019-02-20\"\n","):\n","    '''\n","    Parameters:\n","        start: start date (YYYY-MM-DD). (str)\n","        end: end date (YYYY-MM-DD). (str)\n","    Return:\n","        save raw data to \"\\\\H3 Data\\\\Raw Data\\\\\" as csv.\n","    '''\n","    # # Import data from wind and store it as csv.\n","    # for factor in get_factors_list():\n","    #     # Get each factor data from Wind.\n","    #     factor_data = w.wsd(\n","    #         get_hs300_stocks_list(),\n","    #         factor,\n","    #         start,\n","    #         end,\n","    #         \"Period=M\",\n","    #         usedf=True  # use pandas dataframe.\n","    #     )[1]            # the result is a tuple and we only need [1].\n","    #     # Name the data file by it's factor string.\n","    #     file_path = path + \"\\\\H3 Data\\\\Raw Data\\\\\" + factor + \".csv\"\n","    #     factor_data.to_csv(file_path)  # store data.\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# factor_data_fetching_and_storing()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def sw_industry_data_fetching_and_storing():\n","    '''\n","    Return: save SHENWAN industry data to \"\\\\H3 Data\\\\Raw Data\\\\\" as csv.\n","    '''\n","    # industry_sw = w.wsd(\n","    #     get_hs300_stocks_list(),\n","    #     \"industry_sw\",\n","    #     \"2019-02-20\",\n","    #     \"2019-02-20\",  # set the start and end date as the same.\n","    #     \"industryType=1;Period=M\",\n","    #     usedf=True\n","    # )[1]\n","    # file_path = path + \"\\\\H3 Data\\\\Raw Data\\\\industry_sw.csv\"\n","    # industry_sw.to_csv(file_path)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# sw_industry_data_fetching_and_storing()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["\n"," > ÔºàÊ≥®ÔºöRet1M, Ret3M, Ret6MÁöÜÁî±PCT_CHGÂêàÊàêÔºõRealizedVol_3M, RealizedVol_6MÁöÜÁî±UNDERLYINGHISVOL_90D‰ª£Êõø„ÄÇÔºâ\n"," >\n"," > Êï∞ÊçÆÊù•Ê∫ê‰∏∫‰∏áÂæ∑ÈáëËûçÊï∞ÊçÆÂ∫ìÔºåÈÄöËøáWindPy APIËé∑Âèñ„ÄÇ\n"," >\n"," > ÂÖ∂‰∏≠‚ÄúÊúÄÁªàÁ°ÆÂÆöÂõ†Â≠ê‚ÄùÂàóÂç≥‰∏∫ÂÖ∂‰∏áÂæ∑ÊåáÊ†áÂ≠óÊÆµÂêç„ÄÇ\n"," >\n"," > ÔºàÊï∞ÊçÆ‰øùÂ≠òÂú®‚ÄúH3 Data‚Äù (\"HS300 Data\" ÁöÑÁº©ÂÜô) Êñá‰ª∂Â§π‰∏≠ÔºåÊ†ºÂºè‰∏∫CSVÔºåÁõ¥Êé•Áî®ÂÖ®Â∞èÂÜôÁöÑ‰∏áÂæ∑ÊåáÊ†áÂêçÂëΩÂêç„ÄÇ\n"," > Âç≥ \"<‰∏áÂæ∑ÊåáÊ†áÂêç>.csv\"ÔºåÂ¶Ç \"pe_ttm.csv\"Ôºâ\n"," >\n"," > Ëé∑ÂèñÁöÑÂéüÂßãÊï∞ÊçÆÂÇ®Â≠òÂú®\"H3 Data/Raw Data\"Êñá‰ª∂Â§πÈáå„ÄÇ\n","\n"," Êï∞ÊçÆÊ†ºÂºèÂ¶Ç‰∏ãÔºö\n","\n"," Ë°å/Âàó | ËÇ°Á•®‰ª£Âè∑Ôºà000001.SZÔºâ\n"," :--|--:\n"," ‰∫§ÊòìÊó•ÊúüÔºàYYYYMMDDÔºâ | Áõ∏Â∫îÂõ†Â≠êÊö¥Èú≤"],"metadata":{}},{"cell_type":"markdown","source":[" # Step 2ÔºöÂõ†Â≠êÊï∞ÊçÆÂ§ÑÁêÜ\n","\n"," > ÂØπÂõ†Â≠êÊï∞ÊçÆËøõË°åÂ§ÑÁêÜ„ÄÇ"],"metadata":{}},{"source":["\n","\n","def get_data(\n","    factor_name, \n","    category=\"Raw\", \n","    start_year=\"2009\"\n","):\n","    '''\n","    Parameter:\n","        factor_name: name of factors in Wind. (str)\n","        category: which category of data. (str)\n","            - \"Raw\"\n","            - \"Processed\"\n","            - \"Neutralized\"\n","        start_year: the year when data start. (str) \n","    Return:\n","        forward-filled factor data. (pd.DataFrame)\n","            index: months. (np.int64)\n","            columns: stocks code list. (str)\n","    '''\n","    data = pd.read_csv(open(\n","        # Extract raw data.\n","        path + \"\\\\H3 Data\\\\\" + category + \" Data\\\\\" + factor_name + \".csv\",\n","        # read-only mode for data protection.\n","        'r',\n","        encoding=\"utf-8\"\n","    ), index_col=[0])\n","\n","    # Forward-fill nan to make quarter report fill the month.\n","    data.fillna(method='ffill', inplace=True)\n","\n","    if factor_name == \"industry_sw\":\n","        pass\n","\n","    else:\n","        # Composite-data's date is formated already.\n","        # There'll be a wired bug if you insist to format again. \n","        # All of the date would be \"1990-01-01\". \n","        if (category==\"Raw\") & (\n","            factor_name not in [\n","                \"pct_chg_1m\", \n","                \"pct_chg_3m\", \n","                \"pct_chg_6m\"\n","            ]\n","        ):\n","            # Make all date format in the same way.\n","            data.index = pd.to_datetime(data.index).strftime('%Y%m%d')\n","        data = data.loc[start_year+'0131' : '20190131']\n","    return data\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def pct_chg_composition():\n","    '''\n","    Return: composite and store pct_chg_3m, pct_chg_6m factor data.\n","    '''\n","    # Turn percentage format from percent to decimal.\n","    pct_chg_data = get_data(\"pct_chg\")/100 + 1\n","\n","    pct_chg_1m = pct_chg_data - 1\n","    pct_chg_3m = pct_chg_data.rolling(3).apply(lambda x: np.prod(x)) - 1\n","    pct_chg_6m = pct_chg_data.rolling(6).apply(lambda x: np.prod(x)) - 1\n","\n","    for factor_data, factor_name in zip(\n","        [pct_chg_1m, pct_chg_3m, pct_chg_6m],\n","        [\"pct_chg_1m\", \"pct_chg_3m\", \"pct_chg_6m\"]\n","    ):\n","        factor_data.index = pd.to_datetime(\n","            factor_data.index\n","        ).strftime('%Y%m%d')\n","\n","        factor_data.to_csv(\n","            path\n","            + \"\\\\H3 Data\\\\Raw Data\\\\\"\n","            + factor_name\n","            + \".csv\"\n","        )\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# pct_chg_composition()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def get_values(data):\n","    '''\n","    Parameter:\n","        data: input data. (pd.DataFrame)\n","    Return:\n","        a list of all values in data except nan. (list)\n","    '''\n","    # Collect all non-nan value into data_list.\n","    value_list = []\n","    for i in range(len(data.columns)):\n","        # is there a way to avoid loop?\n","        value_list += data.iloc[:, i].dropna().tolist()\n","    return value_list\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Â¶ÇÂõæ‰∏∫‰ªªÂèñ9‰∏™Âõ†Â≠êÁöÑÊ≤™Ê∑±300ÁöÑÊö¥Èú≤Êï∞ÊçÆÂú®2005~2018Âπ¥ÂàÜÂ∏ÉÁªüËÆ°Âõæ„ÄÇüëá"],"metadata":{}},{"source":["\n","\n","def overview(\n","    source_data_function,\n","    title\n","):\n","    '''\n","    Parameters:\n","        source_data_function: \n","            the function to get source data for plot. (function)\n","        title: \n","            the title of the plot as well as the file. (str)\n","    Return: save a 3*3 histogram distribution plot of data.\n","    '''\n","    factors_list = get_factors_list()[:9]\n","\n","    plt.figure(figsize=(10, 10))\n","    for i, factor in zip(\n","        range(len(factors_list)),\n","        factors_list\n","    ):\n","        plt.subplot(int(\"33\" + str(i+1)))\n","        sns.distplot(get_values(\n","            data=source_data_function(factor)\n","        ))\n","        plt.title(factor)\n","\n","    plt.suptitle(title)\n","    plt.savefig(path + \"\\\\H3 Plots\\\\\" + title + \".png\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview(\n","    source_data_function=get_data,  \n","    title=\"‰∏çÂêåÂõ†Â≠êÂú®AËÇ°ÁöÑÂéÜÂè≤Êï∞ÊçÆÂàÜÂ∏É\"\n",")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ‰ªéÂõæ‰∏≠ÂèØ‰ª•ÁúãÂá∫ÂéüÂßãÁöÑÂõ†Â≠êÊï∞ÊçÆÈÉΩÂ≠òÂú®ÊûÅÂ∑ÆËøáÂ§ß„ÄÅÂàÜÂ∏ÉÈùûÂ∏∏‰∏çÂùáÂåÄÁöÑÁé∞Ë±°„ÄÇ\n"," Â§ßÂ§öÊï∞Êï∞ÊçÆÈõÜ‰∏≠‰∫é‰∏Ä‰∏™ÂÄºÈôÑËøëÔºå‰ΩÜÊòØÊÄª‰ΩìÊù•ÁúãÂÄºÂüüÂèàÈùûÂ∏∏Âπø„ÄÇ\n","\n"," ËøáÂ§ßÊàñËøáÂ∞èÁöÑÊï∞ÊçÆÈÉΩ‰ºöÂΩ±ÂìçÂà∞ÁªüËÆ°ÂàÜÊûêÁöÑÁªìÊûúÔºåÊâÄ‰ª•ÈúÄË¶ÅÂØπÊï∞ÊçÆËøõË°åÂ§ÑÁêÜ„ÄÇ\n","\n"," ## 2.1 Â°´Ë°•Áº∫Â§±ÂÄº\n","\n"," Áî±‰∫é‰∏áÂæ∑ËæìÂá∫ÁöÑÂΩìÂ≠£Â∫¶Ë¥¢Âä°Êï∞ÊçÆÂè™Âú®Êä•ÂëäÊúüÊúâÊï∞ÊçÆÔºåËÄåÂú®ËØ•Â≠£Â∫¶ÁöÑÂÖ∂‰ªñÊúà‰ªΩÊ≤°ÊúâÊï∞ÊçÆÔºåÊâÄ‰ª•ÈíàÂØπËøô‰∏™Áé∞Ë±°ÈááÁî®‚Äú**ÂêëÂâçÂ°´ÂÖÖ**‚ÄùÊù•Â°´Ë°•Áº∫Â§±ÂÄº„ÄÇ\n","\n"," ```Python3\n"," data.fillna(method = 'ffill', inplace = True)\n"," ```\n"," ÈíàÂØπÂâ©‰ΩôÁöÑÁº∫Â§±Êï∞ÊçÆÔºåÊàë‰ª¨Â∞ÜÂú®Êï∞ÊçÆ[Ê†áÂáÜÂåñ](##2.3Ê†áÂáÜÂåñ)Â§ÑÁêÜÂêéÁªü‰∏ÄÂ°´ÂÖÖ‰∏∫Èõ∂„ÄÇ\n","\n"," ## 2.2 ÂéªÊûÅÂÄº\n","\n"," ÂéªÊûÅÂÄºÁöÑÊñπÊ≥ïÈááÁî®Ë∞ÉÊï¥Âõ†Â≠êÂÄº‰∏≠ÁöÑÁ¶ªÁæ§ÂÄºËá≥ÊåáÂÆöÈòàÂÄºÁöÑ‰∏ä‰∏ãÈôêÔºå‰ªéËÄåÂáèÂ∞è**Á¶ªÁæ§ÂÄº**Âíå**ÊûÅÂÄº**ÂØπÁªüËÆ°ÁöÑÂÅèÂ∑Æ„ÄÇ\n","\n"," Á¶ªÁæ§ÂÄºÁöÑÈòàÂÄº‰∏ä‰∏ãÈôêÂÆö‰πâÁöÑÊñπÊ≥ï‰∏ªË¶ÅÊúâ‰∏âÁßçÔºö\n","\n"," 1. MADÊ≥ï\n"," 2. 3œÉÊ≥ï\n"," 3. ÁôæÂàÜ‰ΩçÊ≥ï\n","\n"," ### 2.2.1 MADÊ≥ï (Median Absolute Deviation)\n","\n"," ÂèñÂõ†Â≠êÁöÑ‰∏≠‰ΩçÊï∞ÔºåÂä†ÂáèÊØè‰∏™Âõ†Â≠ê‰∏éËØ•‰∏≠‰ΩçÊï∞ÁöÑÁªùÂØπÂÅèÂ∑ÆÂÄºÁöÑ‰∏≠‰ΩçÊï∞‰πò‰∏äÁªôÂÆöÂèÇÊï∞ÔºàÊ≠§Â§ÑÁªèËøáË∞ÉÂèÇËÆæÂÆöÈªòËÆ§‰∏∫100ÂÄçÔºâÂæóÂà∞‰∏ä‰∏ãÈòàÂÄº„ÄÇ\n","\n"," ÁªèËøáMADÊ≥ïÂéªÊûÅÂÄºÂêéÁöÑÂõ†Â≠êÊï∞ÊçÆÊ¶ÇËßàÂ¶Ç‰∏ãÔºö"],"metadata":{}},{"source":["\n","\n","def MAD_filter(factor_name, n=60):\n","    '''\n","    Parameter:\n","        factor_name: name of factors in Wind. (str)\n","        n: how many times new median. (int)\n","    Return:\n","        filtered data. (pd.DataFrame)\n","    '''\n","    data = get_data(factor_name)\n","    values = get_values(data)\n","    median = np.percentile(\n","        values,\n","        50\n","    )\n","    new_median = np.percentile(\n","        get_values(abs(data - median)), 50\n","    )\n","    min_range = median - n * new_median\n","    max_range = median + n * new_median\n","    return data.clip(min_range, max_range, axis=1)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview(\n","    source_data_function=MAD_filter,\n","    title=\"ÁªùÂØπÂÄºÂ∑Æ‰∏≠‰ΩçÊï∞Ê≥ï(MADÊ≥ï)ÂéªÊûÅÂÄºÂêé\"\n",")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### 2.2.2 3œÉÊ≥ï\n","\n"," ÂèñÊâÄÊúâÂõ†Â≠êÊï∞ÊçÆÁöÑÊ†áÂáÜÂ∑ÆÔºàÂç≥œÉÔºâÔºåÂÅèÁ¶ªÂπ≥ÂùáÂÄºÁªôÂÆöÂèÇÊï∞ÔºàÊ≠§Â§ÑÈªòËÆ§‰∏∫‰∏âÂÄçÔºâÊ†áÂáÜÂ∑ÆÂ§ÑËÆæ‰∏∫‰∏ä‰∏ãÈòàÂÄº„ÄÇ\n","\n"," ÁªèËøá3œÉÊ≥ïÂéªÊûÅÂÄºÂêéÁöÑÂõ†Â≠êÊï∞ÊçÆÊ¶ÇËßàÂ¶Ç‰∏ãÔºö"],"metadata":{}},{"source":["\n","\n","def three_sigma_filter(factor_name, n=3):\n","    '''\n","    Parameter:\n","        factor_name: name of factors in Wind. (str)\n","        n: how many sigmas. (int)\n","    Return:\n","        filtered data. (pd.DataFrame)\n","    '''\n","    data = get_data(factor_name)\n","    values = get_values(data)\n","    min_range = np.mean(values) - n * np.std(values)\n","    max_range = np.mean(values) + n * np.std(values)\n","    return data.clip(min_range, max_range, axis=1)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview(\n","    source_data_function=three_sigma_filter, \n","    title=\"3œÉÊ≥ïÂéªÊûÅÂÄºÂêé\"\n",")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### 2.2.3 ÁôæÂàÜ‰ΩçÊ≥ï\n","\n"," ÂèñÁªôÂÆöÁôæÂàÜ‰Ωç‰Ωú‰∏∫‰∏ä‰∏ãÈòàÂÄº„ÄÇÔºàÊ≠§Â§ÑÁªèËøáË∞ÉÂèÇËÆæÂÆö‰∏∫‰∏ãÈôê1.5%Ôºå‰∏äÈôê98.5%ÂàÜ‰ΩçÁÇπÔºâ\n","\n"," ÁªèËøáÁôæÂàÜ‰ΩçÊ≥ïÂéªÊûÅÂÄºÂêéÁöÑÂõ†Â≠êÊï∞ÊçÆÊ¶ÇËßàÂ¶Ç‰∏ãÔºö"],"metadata":{}},{"source":["\n","\n","def percentile_filter(factor_name, min=1.5, max=98.5):\n","    '''\n","    Parameters:\n","        factor_name: name of factors in Wind. (str)\n","        min: minimum percentage. (float)\n","        max: maximum percentage. (float)\n","    Return:\n","        filtered data. (pd.DataFrame)\n","    '''\n","    data = get_data(factor_name)\n","    values = get_values(data)\n","    min_range = np.percentile(values, min)\n","    max_range = np.percentile(values, max)\n","    return np.clip(data, min_range, max_range)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview(\n","    source_data_function=percentile_filter, \n","    title=\"ÁôæÂàÜ‰ΩçÊ≥ïÂéªÊûÅÂÄºÂêé\"\n",")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### 2.2.4 ÂéªÊûÅÂÄºÁ†îÁ©∂„ÄÇ\n","\n"," ÂÆûÈôÖ‰∏äÔºåÂç≥‰ΩøÁªèËøáË∞ÉÂèÇÂ∞ΩÂèØËÉΩÂú∞‰Ωø‰∏âÁßç‰∏ªÊµÅÁöÑÂéªÊûÅÂÄºÊñπÊ≥ïÁöÑÁªìÊûú‰∫íÁõ∏Êé•ËøëÔºåÂπ∂‰∏çËá≥‰∫éÂá∫Áé∞Ëøá‰∫éÈõÜ‰∏≠ÁöÑÈòàÂÄºÔºå‰ªçÁÑ∂ÊúâÂèØËÉΩÂá∫Áé∞ÈùûÂ∏∏ÊòæËëó‰∏çÂêåÁöÑÊïàÊûú„ÄÇ\n","\n"," ‰ª•ÊØèËÇ°Áé∞ÈáëÊµÅ‰∏∫‰æãÔºåÂ∞ÜÂéüÂßãÊï∞ÊçÆÂíå‰∏âÁßçÂéªÊûÅÂÄºÁöÑÊñπÊ≥ïÂ§ÑÁêÜÂêéÁöÑÂõ†Â≠êÊï∞ÊçÆÊîæÂú®Âêå‰∏ÄÂº†ÂõæÈáåÔºåÁî±‰∫éÂÄºÂüüÁõ∏Â∑ÆÂ§™Â§ßÔºåÁîöËá≥Ê†πÊú¨Êó†Ê≥ï‰ªéÂõæ‰∏≠ÊâæÂà∞‰∏çÂêåÁöÑÊñπÊ≥ïÂØπÂ∫îÁöÑÂõæË°®„ÄÇÔºàÂ¶Ç‰∏ãÂõæÔºöÂàÜÂà´ÈááÁî®‰∏âÁßçÂéªÊûÅÂÄºÊñπÊ≥ïÂ§ÑÁêÜÂêéÁöÑÊØèËÇ°Áé∞ÈáëÊµÅÊï∞ÊçÆ‰∏éÂÖ∂ÂéüÂßãÊï∞ÊçÆÂõæüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def huge_deviation_filter_method_comparison(factor_name):\n","    '''\n","    Return:\n","        save a histogram distribution plot of \n","        a hugely deviated data for different filter method comparison.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.distplot(get_values(\n","        data=get_data(factor_name)\n","    ), label=\"Original\")\n","    sns.distplot(get_values(\n","        data=MAD_filter(factor_name)\n","    ), label=\"MAD\")\n","    sns.distplot(get_values(\n","        data=three_sigma_filter(factor_name)\n","    ), label=\"3œÉ\")\n","    sns.distplot(get_values(\n","        data=percentile_filter(factor_name)\n","    ), label=\"Percentile\")\n","    plt.legend()\n","    plt.title(\"‰∏çÂêåÂéªÊûÅÂÄºÊñπÊ≥ïÁöÑÊØîËæÉÔºà‰ª•Ëê•‰∏öÊî∂ÂÖ•ÂêåÊØîÂ¢ûÈïøÁéá‰∏∫‰æãÔºâ\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\Comparison\" + factor_name + \".png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["huge_deviation_filter_method_comparison(\"yoy_or\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Á©∂ÂÖ∂ÂéüÂõ†ÔºåÊòØÂÖ∂ÂéüÂßãÊï∞ÊçÆÁöÑÈõÜ‰∏≠Â∫¶Â∞±ÈùûÂ∏∏È´òÔºå‰ª•Ëá≥‰∫é‰∏çÂêåÊñπÊ≥ïÂéªÊûÅÂÄºËÆ°ÁÆóÂá∫Áõ∏Â∑ÆÁîöËøúÁöÑÈòàÂÄº„ÄÇÔºàÂ¶Ç‰∏ãÂõæÔºöÂÖ®ÈÉ®AËÇ°Ê†∑Êú¨ÊúüÂÜÖÊØèËÇ°Áé∞ÈáëÊµÅÁöÑÂØÜÂ∫¶ÂàÜÂ∏ÉÂõæüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def huge_deviation_original_data():\n","    '''\n","    Return:\n","        save a histogram distribution plot of \n","        original data with huge deviation.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.distplot(get_values(\n","        data=get_data(\"yoy_or\")\n","    ), label=\"Percentile\")\n","    plt.legend()\n","    plt.title(\"Ëê•‰∏öÊî∂ÂÖ•ÂêåÊØîÂ¢ûÈïøÁéáÔºöÂéüÂßãÊï∞ÊçÆ\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\original yoy_or.png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["huge_deviation_original_data()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÊâÄ‰ª•ÁªèËøáÁôæÂàÜ‰ΩçÂéªÊûÅÂÄºÂêéÔºåÂ∞ΩÁÆ°ÂÄºÂüüÁº©Â∞è‰∫ÜËøë100ÂÄçÔºå‰ΩÜ‰ªçÁÑ∂ÈùûÂ∏∏ÈõÜ‰∏≠„ÄÇ\n","\n"," Âè¶Â§ñÔºåËøôÁßçÁ¶ªÂ∑ÆËøáÂ§ßÁöÑÊï∞ÊçÆÂéªÊûÅÂÄºÁöÑÊó∂ÂÄôËøò‰ºöÂá∫Áé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºöÈÄ†ÊàêÈòàÂÄºÈÉ®ÂàÜÂá∫Áé∞ÂºÇÂ∏∏È´òÁöÑ‚ÄúËôöÂÅá‚ÄùÊï∞ÊçÆÔºåËÄåËøô‰πüÊòØÊàë‰ª¨‰∏çÊÑøÊÑèÁúãÂà∞ÁöÑ„ÄÇÔºàÂ¶Ç‰∏ãÂõæÔºöÊØèËÇ°Áé∞ÈáëÊµÅÁªèËøáÁ∫¶ÊùüÊúÄ‰∏•Ê†ºÁöÑÁôæÂàÜ‰ΩçÂéªÊûÅÂÄºÂ§ÑÁêÜÂêéÁöÑÂàÜÂ∏ÉÂõæüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def huge_deviation_filtered_data():\n","    '''\n","    Return:\n","        save a histogram distribution plot of \n","        percentile-filtered data with huge deviation.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.distplot(get_values(\n","        data=percentile_filter(\"yoy_or\")\n","    ), label=\"Percentile\")\n","    plt.legend()\n","    plt.title(\"Ëê•‰∏öÊî∂ÂÖ•ÂêåÊØîÂ¢ûÈïøÁéáÔºöÁôæÂàÜ‰ΩçÂéªÊûÅÂÄº\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\percentile filter yoy_or.png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["huge_deviation_filtered_data()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" > Ê≥®ÊÑèÂõæ‰∏≠ [-50, 150] Â§ÑÂºÇÂ∏∏ÁöÑ‚ÄúÁ™ÅËµ∑‚Äù„ÄÇ\n"," >\n"," > ËøôÊòØÁî±‰∫éËøáÂ§öË∂ÖÂá∫‰∏ä‰∏ãÈòàÂÄºÁöÑÊï∞ÊçÆË¢´Ëø´Ë∞ÉÊï¥‰∏∫‰∏ä‰∏ãÈòàÂÄºÔºåÂØºËá¥ÈòàÂÄºÂ§ÑÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÁâπÂà´ÂØÜÈõÜ„ÄÇ\n","\n"," ‰ΩÜÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºàÊï∞ÊçÆÂàÜÂ∏ÉÁõ∏ÂØπÂùáÂåÄÊó∂ÔºåÊ≠§Â§Ñ‰ª•ROE‰∏∫‰æãÔºâÔºåÂêÑÁßçÊñπÊ≥ï‰ª•ÂèäÂéüÂßãÊï∞ÊçÆÁõ∏Â∑Æ‰∏çÂ§ß„ÄÇÔºàÂ¶Ç‰∏ãÂõæÔºöËµÑ‰∫ßÂë®ËΩ¨ÁéáÊï∞ÊçÆÁöÑÂéüÂßãÊï∞ÊçÆÂèäÂàÜÂà´ÁªèËøá‰∏âÁßçÂéªÊûÅÂÄºÊñπÊ≥ïÂ§ÑÁêÜÂêéÁöÑÂàÜÂ∏ÉÂõæüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def filter_method_comparison():\n","    '''\n","    Return:\n","        save a histogram distribution plot of\n","        a normal data for different filter method comparison.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.distplot(get_values(\n","        data=get_data(\"roa_ttm2\")\n","    ), label=\"Original\")\n","    sns.distplot(get_values(\n","        data=MAD_filter(\"roa_ttm2\")\n","    ), label=\"MAD\")\n","    sns.distplot(get_values(\n","        data=three_sigma_filter(\"roa_ttm2\")\n","    ), label=\"3œÉ\")\n","    sns.distplot(get_values(\n","        data=percentile_filter(\"roa_ttm2\")\n","    ), label=\"Percentile\")\n","    plt.legend()\n","    plt.title(\"‰∏çÂêåÂéªÊûÅÂÄºÊñπÊ≥ïÁöÑÊØîËæÉÔºà‰ª•ÊÄªËµÑ‰∫ßÂáÄÂà©Áéá‰∏∫‰æãÔºâ\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\Comparison(roa_ttm2).png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["filter_method_comparison()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÁªèËøáÊØîËæÉÁ†îÁ©∂ÔºåÊàë‰ª¨ÊúÄÁªàÈÄâÂèñÈòàÂÄºÈÄâÂèñÁõ∏ÂØπÊúÄ‰∏∫ÂêàÁêÜÔºåËæÉÂ∞ëÈòàÂÄºÂºÇÂ∏∏‚ÄúÁ™ÅËµ∑‚ÄùÔºåÂêåÊó∂‰øùÁïôËæÉÂÆΩÂÄºÂüüÁöÑ**ÂèÇÊï∞ÂÄº‰∏∫60ÁöÑMADÊ≥ï**ËøõË°åÂéªÊûÅÂÄºÂ§ÑÁêÜ„ÄÇ\n","\n"," ## 2.3 Ê†áÂáÜÂåñ\n","\n"," Ê†áÂáÜÂåñÂ§ÑÁêÜÊï∞ÊçÆÁöÑÁõÆÁöÑÂ∞±ÊòØÂéªÈô§ÂÖ∂**ÈáèÁ∫≤**„ÄÇ\n","\n"," ËøôÊ†∑ÂÅöÂèØ‰ª•‰ΩøÂæóÔºö\n","\n"," - Êï∞ÊçÆÊõ¥Âä†ÈõÜ‰∏≠\n"," - ‰∏çÂêåÊï∞ÊçÆ‰πãÈó¥ÂèØ‰ª•‰∫íÁõ∏ÊØîËæÉÂíåËøõË°åÂõûÂΩíÁ≠â\n","\n"," ‰∏ªÊµÅÁöÑÊ†áÂáÜÂåñÁöÑÊñπÊ≥ïÊúâ‰∏§ÁßçÔºö\n","\n"," Ê†áÂáÜÂåñÊñπÊ≥ï|ÂéüÁêÜ|‰ºòÁÇπ|Áº∫ÁÇπ\n"," :--|:--|:--:|:--:\n"," ÂØπÂéüÂßãÂõ†Â≠êÂÄºÊ†áÂáÜÂåñ|ÂáèÂéªÂùáÂÄºÂêéÔºåÈô§‰ª•Ê†áÂáÜÂ∑Æ|‰øùÁïôÊõ¥Â§ö‰ø°ÊÅØ|ÂØπÊï∞ÊçÆÂàÜÂ∏ÉÊúâË¶ÅÊ±Ç\n"," ÂØπÂõ†Â≠êÊéíÂ∫èÂÄºÊ†áÂáÜÂåñ|Âõ†Â≠êÊéíÂ∫èÂÄºËøõË°å‰∏äËø∞Â§ÑÁêÜ|ÈÄÇÁî®ÊÄßÊõ¥ÂπøÊ≥õ|ÈùûÂèÇÊï∞ÁªüËÆ°Ê≥ï\n","\n"," ÂÆÉ‰ª¨ÈÉΩËÉΩ‰ΩøÂæóÊï∞ÊçÆÁöÑÔºö\n","\n"," - ÂùáÂÄº‰∏∫0\n"," - Ê†áÂáÜÂ∑Æ‰∏∫1\n","\n"," Áî±‰∫éÂ∑≤ÁªèÂØπÊï∞ÊçÆËøõË°åÂéªÊûÅÂÄºÂ§ÑÁêÜÔºåÊàë‰ª¨ÊúÄÁªàÈÄâÂèñÂØπÂéüÂßãÂõ†Â≠êÂÄºËøõË°åÊ†áÂáÜÂåñ(z-score)ÁöÑÊñπÊ≥ïËøõË°åÊ†áÂáÜÂåñ„ÄÇ\n","\n"," > 2.1Ôºå 2.2Ôºå 2.3ÁöÑÊï∞ÊçÆÂ§ÑÁêÜÈÉ®ÂàÜÁöÑÔºö\n"," >\n"," > Êï∞ÊçÆ‰øùÂ≠òÂú®\"H3 Data/Processed Data\"Êñá‰ª∂Â§πÈáå„ÄÇ"],"metadata":{}},{"source":["# Use z-score method to standardize.\n","\n","\n","def standardize(factor_name):\n","    '''\n","    Parameter:\n","        factor_name: name of factors in Wind. (str)\n","        start_year:the start_year the data start\n","    Return:\n","        standardized and Filtered (MAD) data. (pd.DataFrame)\n","    '''\n","    data = MAD_filter(factor_name)\n","    # data = data.fillna(0)\n","    mean = np.mean(data)\n","    std = np.std(data)\n","    return (data - mean) / std\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def process_and_store_data():\n","    '''\n","    Return:\n","        save processed data in \"\\\\H3 Data\\\\Processed Data\\\\\".\n","        (\"processed\" means filtered & standardized.)\n","    '''\n","    for factor in get_factors_list():\n","        processed_data = standardize(factor)\n","        file_path = path + \"\\\\H3 Data\\\\Processed Data\\\\\" + factor + \".csv\"\n","        processed_data.to_csv(file_path)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# process_and_store_data()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["\n"," ÔºàÂ¶Ç‰∏ãÂõæ‰∏∫ÁªèËøáÂéªÊûÅÂÄº„ÄÅÊ†áÂáÜÂåñÂ§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÂØÜÂ∫¶ÂàÜÂ∏ÉÂõæ‰∏ÄËßàüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def overview_processed_data():\n","    # Get an overview of processed data.\n","    plt.figure(figsize=(10, 10))\n","    for i in range(9):\n","        plt.subplot(int(\"33\" + str(i+1)))\n","        sns.distplot(get_values(\n","            data=get_data(get_factors_list()[i], category=\"Processed\")\n","        ))\n","        plt.title(get_factors_list()[i])\n","    plt.suptitle(\"ÁªèËøáÂ§ÑÁêÜÂêéÁöÑAËÇ°Âõ†Â≠êÊï∞ÊçÆÂØÜÂ∫¶ÂàÜÂ∏ÉÂõæ‰∏ÄËßà\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\Processed Data.png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview_processed_data()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## 2.4 ‰∏≠ÊÄßÂåñ\n","\n"," ‰∏≠ÊÄßÂåñÁöÑÁõÆÁöÑÊòØÂâîÈô§Êï∞ÊçÆ‰∏≠Â§ö‰ΩôÁöÑÈ£éÈô©Êö¥Èú≤„ÄÇ\n","\n"," Ê†πÊçÆÊüê‰∫õÂõ†Â≠êÔºàÊåáÊ†áÔºâÈÄâËÇ°ÁöÑÊó∂ÂÄôÔºåÁî±‰∫éÊüê‰∫õÂõ†Â≠ê‰πãÈó¥ÂÖ∑ÊúâËæÉÂº∫ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊïÖÊó∂Â∏∏‰ºöÊúâÊàë‰ª¨‰∏çÂ∏åÊúõÁúãÂà∞ÁöÑ‚Äú**ÂÅèÂêë**‚ÄùÔºåÂØºËá¥ÊäïËµÑÁªÑÂêà‰∏çÂ§ü**ÂàÜÊï£**„ÄÇ\n","\n"," ‰æãÂ¶Ç‰ª•‰∏ãÂõõ‰∏™ÊåáÊ†áÔºö\n","\n"," - Â∏ÇÁé∞Áéá\n"," - ÂáÄÂà©Ê∂¶ÂêåÊØîÂ¢ûÈïøÁéá\n"," - ÂáÄËµÑ‰∫ßÊî∂ÁõäÁéáÂêåÊØîÂ¢ûÈïøÁéá\n"," - Â≠òË¥ßÂë®ËΩ¨Áéá"],"metadata":{}},{"source":["\n","\n","def get_industry_list():\n","    '''\n","    Return:\n","        industry list in HS300 stocks list.\n","    '''\n","    return list(get_data(\"industry_sw\").iloc[:, 0].unique())\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def industry_comparison(factor_name):\n","    '''\n","    Parameter:\n","        factor_name: name of factors in Wind. (str)\n","    Return:\n","        average factor value of each industries. (pd.DataFrame)\n","            index: industry. (str)\n","            columns: factor name. (str)\n","    '''\n","    # All industry in HS300.\n","    # Use certain factor data for comparison example between industry.\n","    compare_data = get_data(factor_name, start_year='2009')\n","    compare_industry = pd.DataFrame(\n","        index=get_industry_list(),\n","        columns=[factor_name]\n","    )\n","    for industry in get_industry_list():\n","        industry_stock_code_list = list(get_data(\"industry_sw\")[\n","            get_data(\"industry_sw\").iloc[:, 0] == industry  \n","        ].index)\n","        # Some industry is not in HS300.\n","        try:\n","            industry_data = compare_data[industry_stock_code_list]\n","            compare_industry.loc[\n","                industry, factor_name\n","            ] = np.mean(np.mean(industry_data))\n","        except:\n","            continue\n","    compare_industry.dropna(inplace=True)\n","    return compare_industry\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Ê≤™Ê∑±300ËÇ°Á•®ÊåáÊï∞‰∏≠ÂÖ±ÂåÖÂê´17‰∏™Ë°å‰∏öÔºàÊ†πÊçÆÁî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÂàÜÁ±ªÔºâÔºåÂàÜÂà´ÁªüËÆ°Ê≤™Ê∑±300ÊåáÊï∞‰∏≠ÂêÑË°å‰∏ö‰ª•‰∏äÂõõ‰∏™ÊåáÊ†áÁöÑÂπ≥ÂùáÂÄºÔºåÁªìÊûúÂ¶Ç‰∏ãÂõæÊâÄÁ§∫üëá„ÄÇ"],"metadata":{}},{"source":["\n","\n","def plot_industry_comparison():\n","    '''\n","    Return:\n","        save a 2*2 plot of average factor of each industries, \n","        which are all siginificantly different. \n","    '''\n","    # Choose 4 factors that's significantly different among industries.\n","    significant_comparison_industry_list = [\n","        \"pcf_ncf_ttm\",\n","        \"yoyprofit\",\n","        \"yoyroe\",\n","        \"invturn\"\n","    ]\n","    plt.figure(figsize=(21, 18))  # it's a big plot.\n","    for i in range(len(significant_comparison_industry_list)):\n","        plot_data = industry_comparison(\n","            significant_comparison_industry_list[i]\n","        )\n","        plt.subplot(int(\"22\" + str(i+1)))\n","        sns.barplot(\n","            x=plot_data.index,\n","            y=significant_comparison_industry_list[i],\n","            data=plot_data\n","        )\n","        plt.xticks(rotation=60)  # rotate to avoid overlap text.\n","        plt.title(\n","            significant_comparison_industry_list[i],\n","            fontsize=21\n","        )\n","    plt.suptitle(\n","        \"Ê≤™Ê∑±300‰∏≠‰∏çÂêåË°å‰∏öÈÉ®ÂàÜÂõ†Â≠êÂπ≥ÂùáÂÄºÊØîËæÉ\",\n","        fontsize=36\n","    )\n","    plt.savefig(path + \"\\\\H3 Plots\\\\Industry Comparison.png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_industry_comparison()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ‰ªéÂõæ‰∏≠ÂèØ‰ª•ÁúãÂà∞Ôºå‰∏çÂêåË°å‰∏öÁöÑ‰∏çÂêåÊåáÊ†áÁõ∏Â∑ÆÂçÅÂÄç„ÄÅÂçÉÂÄç‰πÉËá≥‰∏áÂÄçÈÉΩÊúâ„ÄÇ\n","\n"," > *ÊúâËâ≤ÈáëÂ±ûË°å‰∏öÁöÑÂπ≥ÂùáÂ∏ÇÁé∞ÁéáÊòØÈì∂Ë°å‰∏öÁöÑËøëË¥üÂõõÂçÅ‰∏áÂÄç„ÄÇ*"],"metadata":{}},{"source":["print(round(\n","    industry_comparison(\"pcf_ncf_ttm\").loc[\"ÊúâËâ≤ÈáëÂ±û\", \"pcf_ncf_ttm\"] /\n","    industry_comparison(\"pcf_ncf_ttm\").loc[\"ÂÆ∂Áî®ÁîµÂô®\", \"pcf_ncf_ttm\"],\n","    0\n","))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÈÇ£‰πàÔºå‰æùÊçÆÂ∏ÇÁé∞ÁéáÂõ†Â≠êÈÄâÂèñÂá∫ÁöÑËÇ°Á•®ÂøÖÁÑ∂ÂØπÂπ≥ÂùáÂ∏ÇÁé∞ÁéáÈ´òÁöÑË°å‰∏öÊúâÂÅèÂêëÔºåËÄåÊàë‰ª¨Â∏åÊúõÊäïËµÑÁªÑÂêà‰∏≠ÁöÑË°å‰∏öÂ∞ΩÂèØËÉΩÂàÜÊï£ÔºåÊïÖÊàë‰ª¨Â∏åÊúõÂØπË°å‰∏öËøõË°å‰∏≠ÊÄßÂåñ„ÄÇÔºàÂêåÁêÜÔºåÊàë‰ª¨‰πüÂ∏åÊúõÂØπÂ∏ÇÂÄºËøõË°å‰∏≠ÊÄßÂåñ„ÄÇÔºâ\n","\n"," ‰∏≠ÊÄßÂåñÁöÑ‰∏ªË¶ÅÂÅöÊ≥ïÂ∞±ÊòØÈÄöËøáÂõûÂΩíÂæóÂà∞‰∏Ä‰∏™‰∏éÈ£éÈô©Âõ†Â≠êÔºàË°å‰∏öÂõ†Â≠ê„ÄÅÂ∏ÇÂÄºÂõ†Â≠êÔºâ**Á∫øÊÄßÊó†ÂÖ≥**ÁöÑÂõ†Â≠ê„ÄÇÔºàÂç≥Á∫øÊÄßÂõûÂΩíÂêéÁöÑÊÆãÂ∑ÆÈ°π‰Ωú‰∏∫‰∏≠ÊÄßÂåñÂêéÁöÑÊñ∞Âõ†Â≠ê„ÄÇÔºâÂ¶ÇÊ≠§‰∏ÄÊù•Ôºå‰∏≠ÊÄßÂåñÂ§ÑÁêÜÂêéÁöÑÂõ†Â≠ê‰∏éÈ£éÈô©Âõ†Â≠ê‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂ∞±‰∏•Ê†º‰∏∫Èõ∂„ÄÇ\n","\n"," > ‰∏çËøáËøôÊ†∑ÂÅö‰∏≠ÊÄßÂåñÂπ∂‰∏ç‰∏ÄÂÆöÊÄªËÉΩÂΩªÂ∫ïÂú∞ÂâîÈô§Âõ†Â≠êÁöÑÂ§ö‰Ωô‰ø°ÊÅØ„ÄÇÂõ†‰∏∫Á∫øÊÄßÂõûÂΩíË¶ÅÊ±Ç‰∏§‰∏™ÂâçÊèêÂÅáËÆæÔºö\n"," >\n"," > - Âõ†Â≠ê‰πãÈó¥Á∫øÊÄßÁõ∏ÂÖ≥\n"," > - ÊÆãÂ∑ÆÊ≠£ÊÄÅÁã¨Á´ãÂêåÂàÜÂ∏É\n"," >\n"," > ËÄåÂú®Âõ†Â≠êÊï∞ÊçÆ‰∏≠Ëøô‰∏§‰∏™ÂÅáËÆæÈÉΩ‰∏ç‰∏ÄÂÆöÊàêÁ´ã„ÄÇÔºà‰æãÂ¶ÇÂú®[2.2ÂéªÊûÅÂÄº](##2.2ÂéªÊûÅÂÄº)Ê≠•È™§‰∏≠ÂØÜÂ∫¶ËøáÈ´òÁöÑÈòàÂÄºÂ∞±ÂØπÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÈÄ†Êàê‰∫ÜÁ†¥ÂùèÔºâ\n","\n"," ‰ΩÜÁõ¥ËßÇÁöÑËØ¥ÔºåÊ†πÊçÆ[BrinsonËµÑ‰∫ßÈÖçÁΩÆÂàÜÊûê](https://www.investopedia.com/terms/a/attribution-analysis.asp)Ë∂ÖÈ¢ùÊî∂ÁõäÁêÜËÆ∫Êù•ÁúãÔºåÂ¶ÇÊûúÊäïËµÑÁªÑÂêà‰∏≠È£éÈô©Âõ†Â≠êÈÖçÁΩÆËµÑ‰∫ßÊùÉÈáçÁ≠â‰∫éÂü∫ÂáÜËµÑ‰∫ß‰∏≠ÂÖ∂‰πãÊùÉÈáçÔºåÂàôÂÅöÂà∞‰∫Ü‰∏≠ÊÄßÂåñ„ÄÇ\n","\n"," Ê≠§Â§ÑÁÆÄ‰æøËµ∑ËßÅÔºåÊàë‰ª¨‰æùÁÑ∂ÈááÁî®Á∫øÊÄßÂõûÂΩí‰Ωú‰∏∫‰∏≠ÊÄßÂåñÁöÑÂ§ÑÁêÜÊñπÊ≥ï„ÄÇ\n","\n"," ÂõûÂΩíÊñπÂºèÂ¶Ç‰∏ãÔºö\n","\n"," - Ë¢´Ëß£ÈáäÂèòÈáèÔºöÂâçËø∞Êï∞ÊçÆÂ§ÑÁêÜÂêéÁöÑÂõ†Â≠êÊï∞ÊçÆ\n","\n"," - Ëß£ÈáäÂèòÈáèÔºö\n","\n","   - Â∏ÇÂÄºÂõ†Â≠ê\n","   - Ë°å‰∏öÂõ†Â≠êÔºà‰Ωú‰∏∫ÊåáÁ§∫ÂèòÈáèÔºâ\n","\n"," ÊúÄÁªàÂõûÂΩíÊñπÁ®ãÁöÑ**ÊÆãÂ∑Æ**È°πÂç≥‰∏∫‰∏≠ÊÄßÂåñÂêéÁöÑÂõ†Â≠êÊö¥Èú≤„ÄÇ\n","\n"," ÔºàÂ¶Ç‰∏ãÂõæÔºå‰∏∫ÈÄâÂèñÂõõ‰∏™Âõ†Â≠êÊåáÊ†áËøõË°åË°å‰∏ö‰∏≠ÊÄßÂåñÂâçÂêéÁöÑÁªìÊûúÔºå‰ª•Â±ïÁ§∫‰∏≠ÊÄßÂåñÁöÑ‰∏ÄËà¨ÁªìÊûúüëáÔºåÂèØ‰ª•ÁúãÂá∫‰∏≠ÊÄßÂåñÂØºËá¥ÂàÜÂ∏ÉÊõ¥ÂùáÂåÄ„ÄÅÊõ¥Êé•ËøëÂùáÂÄºÔºâ"],"metadata":{}},{"source":["\n","\n","def get_industry_exposure(factor_name):\n","    '''\n","    Parameter:\n","        factor_name: name of factors in Wind. (str)\n","    Return:\n","        industry exposure data. (pd.DataFrame)\n","    '''\n","    file_path = path + \"\\\\H3 Data\\\\Neutralized Data\\\\industry exposure \" + factor_name + \".csv\"\n","    if os.path.isfile(file_path):\n","        industry_exposure = pd.read_csv(\n","            open(\n","                file_path,\n","                'r',\n","                encoding=\"utf-8\"\n","            ),\n","            index_col=[0]\n","        )\n","    else:\n","        # Don't know why but different factor data \\\n","        # has different hs300 stocks list,\n","        # so specify which factor is essential.\n","        hs300_stock_list = list(get_data(\n","            factor_name, \n","            category=\"Processed\", \n","            start_year='2009'\n","        ).columns)\n","        industry_exposure = pd.DataFrame(\n","            index=get_industry_list(),\n","            columns=hs300_stock_list\n","        )\n","        for stock in hs300_stock_list:\n","            try:\n","                industry_exposure.loc[\n","                    get_data(\"industry_sw\").loc[\n","                        stock,\n","                        \"INDUSTRY_SW\"\n","                    ],\n","                    stock\n","                ] = 1\n","            except:\n","                continue\n","        industry_exposure.fillna(0, inplace=True)\n","        industry_exposure.to_csv(file_path)\n","    return industry_exposure\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def neutralize(\n","    factor_name, \n","    start_year=\"2009\", \n","    market_capital=True,\n","    industry=True\n","):\n","    '''\n","    Parameters:\n","        factor_name: name of factors in Wind. (str)\n","        market_capital: whether market-capital-neutralize or not. (bool)\n","        industry: whether industry-neutralize or not. (bool)\n","    Return:\n","        neutralized data. (pd.DataFrame)\n","    '''\n","    # don't know why but there's still nan.\n","    y = get_data(\n","        factor_name, \n","        category=\"Processed\", \n","        start_year=\"2009\"\n","    ).T.fillna(0)\n","    industry_dummy = get_industry_exposure(factor_name)\n","    if market_capital:\n","        ln_market_capital = get_data(\n","            \"val_lnmv\", \n","            category=\"Processed\", \n","            start_year='2009'\n","        )\n","        if industry:\n","            x = pd.concat(\n","                [\n","                    ln_market_capital,\n","                    industry_dummy\n","                ],\n","                axis=1\n","            ).T\n","        else:\n","            x = ln_market_capital.T\n","    elif industry:\n","        x = industry_dummy.T\n","    \n","    x.fillna(0, inplace=True)\n","    y = y.loc[x.index, :]\n","    \n","    result = sm.OLS(\n","        y.astype(float),\n","        x.astype(float)\n","    ).fit()\n","    \n","    return result.resid.T\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def plot_industry_neutralization(factor_name):\n","    '''\n","    Return: \n","        a plot of neutralization comparison.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.kdeplot(get_values(\n","        data=get_data(factor_name, category=\"Processed\")\n","    ), label=\"Êú™Áªè‰∏≠ÊÄßÂåñ\")\n","    sns.kdeplot(get_values(\n","        data=neutralize(\n","            factor_name,\n","            market_capital=False,\n","            industry=True\n","        )\n","    ), label=\"Ë°å‰∏ö‰∏≠ÊÄßÂåñ\")\n","    plt.legend()\n","    plt.title(\"ÂØπ\" + factor_name + \"ËøõË°å‰∏≠ÊÄßÂåñÂ§ÑÁêÜÂâçÂêéÊØîËæÉ\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\industry neutralization.png\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def overview_industry_neutralization(factor_list):\n","    '''\n","    Parameter:\n","        factor_list: list of factor names. (list)\n","    Return:\n","        save a 2*2 plot of neutralization comparison.\n","    '''\n","    plt.figure(figsize=(10, 10))\n","    for i in range(len(factor_list)):\n","        plt.subplot(int(\"22\" + str(i+1)))\n","        sns.kdeplot(get_values(\n","            data=get_data(factor_list[i], category=\"Processed\")\n","        ), label=\"Êú™Áªè‰∏≠ÊÄßÂåñ\")\n","        sns.kdeplot(get_values(\n","            data=neutralize(\n","                factor_list[i],\n","                market_capital=False,\n","                industry=True\n","            )\n","        ), label=\"Ë°å‰∏ö‰∏≠ÊÄßÂåñ\")\n","        plt.legend()\n","        plt.title(\"ÂØπ\" + factor_list[i] + \"ËøõË°åË°å‰∏ö‰∏≠ÊÄßÂåñÂ§ÑÁêÜÂâçÂêéÊØîËæÉ\")\n","    plt.suptitle(\"Ë°å‰∏ö‰∏≠ÊÄßÂåñÁöÑÂÖ∏ÂûãÁªìÊûú\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\overview industry neutralization.png\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview_industry_neutralization([\n","    \"pb_lf\",\n","    \"debttoassets\",\n","    \"assetsturn\",\n","    \"invturn\"\n","])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÂØπÂ∏ÇÂÄºËøõË°å‰∏≠ÊÄßÂåñ‰πüÊúâÁ±ª‰ººÁöÑÊïàÊûú„ÄÇÔºàÂ¶Ç‰∏ãÂõæ‰∏∫ÂØπ\"pb_lf\"Âõ†Â≠êËøõË°åÂ∏ÇÂÄº‰∏≠ÊÄßÂåñÁöÑÁªìÊûúüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def plot_market_neutralization(factor_name):\n","    '''\n","    Return: \n","        a plot of neutralization comparison.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.kdeplot(get_values(\n","        data=get_data(factor_name, category=\"Processed\")\n","    ), label=\"Êú™Áªè‰∏≠ÊÄßÂåñ\")\n","    sns.kdeplot(get_values(\n","        data=neutralize(\n","            factor_name,\n","            market_capital=True,\n","            industry=False\n","        )\n","    ), label=\"Â∏ÇÂÄº‰∏≠ÊÄßÂåñ\")\n","    plt.legend()\n","    plt.title(\"ÂØπ\" + factor_name + \"ËøõË°åÂ∏ÇÂÄº‰∏≠ÊÄßÂåñÂ§ÑÁêÜÂâçÂêéÊØîËæÉ\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\market neutralization.png\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_market_neutralization(\"pb_lf\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÂêåÊ†∑ÊòØ\"pb_lf\"Âõ†Â≠êÔºåÂêåÊó∂ÂØπÂ∏ÇÂÄºÂíåË°å‰∏öËøõË°å‰∏≠ÊÄßÂåñüëáÔºåÊïàÊûú‰πüÊòØÁõ∏ËøëÁöÑ„ÄÇ"],"metadata":{}},{"source":["\n","\n","def plot_neutralization(factor_name):\n","    '''\n","    Return: \n","        a plot of neutralization comparison.\n","    '''\n","    plt.figure(figsize=(8, 5))\n","    sns.kdeplot(get_values(\n","        data=get_data(factor_name, category=\"Processed\")\n","    ), label=\"Êú™Áªè‰∏≠ÊÄßÂåñ\")\n","    sns.kdeplot(get_values(\n","        data=neutralize(\n","            factor_name,\n","            market_capital=True,\n","            industry=True\n","        )\n","    ), label=\"Ë°å‰∏öÂ∏ÇÂÄº‰∏≠ÊÄßÂåñ\")\n","    plt.legend()\n","    plt.title(\"ÂØπ\" + factor_name + \"ËøõË°åË°å‰∏öÂ∏ÇÂÄº‰∏≠ÊÄßÂåñÂ§ÑÁêÜÂâçÂêéÊØîËæÉ\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\industry & market neutralization.png\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_neutralization(\"pb_lf\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["\n"," > Êï∞ÊçÆÂ§ÑÁêÜ‰∏≠ÊÄßÂåñÈÉ®ÂàÜÁöÑÔºö\n"," >\n"," > Êï∞ÊçÆ‰øùÂ≠òÂú®\"H3 Data/Neutralized Data\"Êñá‰ª∂Â§πÈáå„ÄÇ\n","\n"," ÊúÄÁªàÁªèËøáÊâÄÊúâÂõ†Â≠êÊï∞ÊçÆÂ§ÑÁêÜÊ≠•È™§‰πãÂêéÔºåÂéüÊù•ÁöÑÂõ†Â≠êÊï∞ÊçÆÂàÜÂ∏ÉÂõæÂèò‰∏∫‰∫ÜËøôÊ†∑„ÄÇ\n","\n"," ÔºàÁªèËøáÊâÄÊúâÊï∞ÊçÆÂ§ÑÁêÜÊ≠•È™§ÂêéÁöÑÂõ†Â≠êÊï∞ÊçÆÂØÜÂ∫¶ÂàÜÂ∏ÉÂõæ‰∏ÄËßàüëáÔºâ"],"metadata":{}},{"source":["\n","\n","def neutralize_and_store_data():\n","    '''\n","    Return:\n","        save industry neutralized data in \n","        \"\\\\H3 Data\\\\Neutralized Data\\\\\".\n","    '''\n","    for factor in get_factors_list():\n","        file_path = path + \"\\\\H3 Data\\\\Neutralized Data\\\\\" + factor + \".csv\"\n","        neutralized_data = neutralize(\n","            factor,\n","            market_capital=True,\n","            industry=True\n","        )\n","        neutralized_data.to_csv(file_path)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# neutralize_and_store_data()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def overview_after_data_processing():\n","    # Get an overview of data after processing.\n","    plt.figure(figsize=(10, 10))\n","    for i in range(9):\n","        plt.subplot(int(\"33\" + str(i+1)))\n","        factor_name = get_factors_list()[i]\n","        sns.distplot(get_values(\n","            data=get_data(factor_name, category=\"Neutralized\")\n","        ))\n","        plt.title(factor_name)\n","    plt.suptitle(\"ÁªèËøáÊï∞ÊçÆÂ§ÑÁêÜÂêéÁöÑ‰∏çÂêåÂõ†Â≠êÂú®AËÇ°ÁöÑÂéÜÂè≤Êï∞ÊçÆÂàÜÂ∏É\")\n","    plt.savefig(path + \"\\\\H3 Plots\\\\overview after data processing.png\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["overview_after_data_processing()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Step 3ÔºöÂ§ßÁ±ªÂõ†Â≠êÂêàÊàê\n","\n"," ÂâçÈù¢‰∏§‰∏™Ê≠•È™§Â∑≤ÁªèÊääÈ£éÊ†ºÂõ†Â≠êÁöÑÁªÜÂàÜÁ±ªÂõ†Â≠êÊï∞ÊçÆÁªèËøáÊï∞ÊçÆÂ§ÑÁêÜÂπ∂‰øùÂ≠ò‰∫Ü‰∏ãÊù•Ôºå\n"," Ëøô‰∏ÄÊ≠•ÊääÁªÜÂàÜÁ±ªÂõ†Â≠êÂêàÊàê‰∏∫Â§ßÁ±ªÂõ†Â≠ê„ÄÇ‰ΩøÂæóÊúÄÁªàÂêàÊàêÂêéÂè™Ââ©‰∏ãÔºö\n","\n"," - VALUE\n"," - GROWTH\n"," - PROFIT\n"," - QUALITY\n"," - VOLATILITY\n"," - MOMENTUM\n"," - LIQUIDITY\n","\n"," Ëøô‰∏É‰∏™Âõ†Â≠êÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÂ∞±ÊòØÊûÑÂª∫Ëøô‰∏É‰∏™Âõ†Â≠êÁöÑ**Á∫ØÂõ†Â≠êÁªÑÂêà**„ÄÇ\n","\n"," > ‰ªéËøô‰∏ÄÊ≠•ÂºÄÂßã‰∏∫Êñπ‰æøÊèêÂèñÊï∞ÊçÆÔºåÂ∞ÜÊï∞ÊçÆ‰ªé\"pandas.DataFrame\"\n"," ËΩ¨Êç¢‰∏∫\"pandas.PanelData\"„ÄÇ\n"," >\n"," > Êï∞ÊçÆÊ†ºÂºè‰∏∫Ôºö\n"," >\n"," > - index: stock codes\n"," > - factor names\n"," >\n"," > ÂèØ‰ª•Áî®‰ª•‰∏ãÊñπÊ≥ïÊèêÂèñÁâπÂÆöÊó∂Èó¥ÁöÑÊâÄÊúâÂõ†Â≠êÁöÑÊâÄÊúâËÇ°Á•®Êï∞ÊçÆÔºö\n"," >\n"," > ```Python3\n"," > Large_factor.major_xs(\"20050131\")\n"," > ```\n","\n"," Â§ßÁ±ªÂõ†Â≠êÂêàÊàêÁöÑÊñπÂºèÊòØÈÄöËøáIC_IRÂä†ÊùÉÂêàÊàê„ÄÇ\n","\n"," > Â§ßÁ±ªÂõ†Â≠êÂêàÊàêÈÉ®ÂàÜÁöÑÔºö\n"," >\n"," > Êï∞ÊçÆ‰øùÂ≠òÂú®\"H3 Data/Composition Data\"Êñá‰ª∂Â§πÈáå„ÄÇ"],"metadata":{}},{"source":["# Turn dataframe into panel data.\n","\n","\n","def get_group_data(factor_list, start_year=\"2009\"):\n","    datadict = {}\n","    for i in factor_list:\n","        # This should be the processed data.\n","        df = get_data(\n","            i, \n","            category=\"Neutralized\", \n","            start_year=start_year\n","        )  \n","        datadict[i] = df\n","    panel = pd.Panel(datadict)\n","    return panel\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","class Large_factor_merge(object):\n","    def __init__(self, Large_factor):\n","        if Large_factor == 'VALUE':\n","            list = [\"pe_ttm\", \"pb_lf\", \"pcf_ncf_ttm\", \"ps_ttm\"]\n","\n","        elif Large_factor == 'GROWTH':\n","            list = [\"yoyprofit\", \"yoy_or\", \"yoyroe\"]\n","\n","        elif Large_factor == 'PROFIT':\n","            list = [\"roe_ttm2\", \"roa_ttm2\"]\n","\n","        elif Large_factor == 'QUALITY':\n","            list = [\"debttoassets\", \"assetsturn\", \"invturn\"]\n","\n","        elif Large_factor == 'MOMENTUM':\n","            list = ['pct_chg_1m', 'pct_chg_3m', 'pct_chg_6m']\n","\n","        elif Large_factor == 'VOLATILITY':\n","            list = [\"stdevry_3m\", \"stdevry_6m\"]\n","\n","        elif Large_factor == 'LIQUIDITY':\n","            list = [\"tech_turnoverrate60\", \"tech_turnoverrate20\"]\n","\n","        self.data = get_group_data(list, \"2007\")\n","        self.data_2009 = get_group_data(list, \"2009\")\n","        self.Large_factor = Large_factor\n","    # Define the following function for you can read clearly \n","    # and can acquire the data of every step.\n","\n","    def Caculate_IC(self):\n","        stock_return = get_data(\n","            \"pct_chg_1m\", \n","            category=\"Neutralized\", \n","            start_year=\"2007\"\n","        )  # This will be modified\n","        datadict = {}\n","        for i in self.data.items:\n","            df = self.data[i]\n","            IC = pd.DataFrame(columns=['IC_monthly'],\n","                              index=df.index[0:len(df)-1])\n","            IC_group = []\n","            for j in range(len(df)-1):\n","                cor = df.iloc[j].corr(stock_return.iloc[j+1])\n","                IC_group.append(cor)\n","            IC['IC_monthly'] = IC_group\n","            datadict[i] = IC\n","        IC_Large = pd.Panel(datadict)\n","        return IC_Large\n","\n","    def Factors_merge_Static(self):\n","        IC_Large = self.Caculate_IC()\n","        weight_df = pd.DataFrame(\n","            columns=['weights'], \n","            index=self.data.items\n","        )\n","        weight = []\n","        for i in IC_Large.items:\n","            df = IC_Large[i]\n","            IR = df.iloc[-24:, 0].mean()/df.iloc[-24:, 0].std()\n","            weight.append(IR)\n","        #weight = [x / sum(weight) for x in weight]  # adjust the sum of weight to 1.0\n","        weight_df['weights'] = weight\n","        weight = weight_df\n","        Factors_sum = pd.DataFrame(\n","            0, \n","            columns=self.data_2009.minor_axis, \n","            index=self.data_2009.major_axis\n","        )\n","        for i in self.data.items:\n","            df = self.data_2009[i]\n","            new_df = df * weight.loc[i, 'weights']\n","            Factors_sum = Factors_sum + new_df\n","        return Factors_sum\n","\n","    def Factors_merge_dynamic(self):\n","        IC_Large = self.Caculate_IC()\n","        weight_df = pd.DataFrame(columns=IC_Large.major_axis[24:], index=IC_Large.items)\n","        for i in IC_Large.items:\n","            for j in range(24, len(IC_Large.major_axis)):\n","                df = IC_Large[i]\n","                IR = df.iloc[j - 23:j+1, 0].mean() / df.iloc[j - 23:j+1, 0].std()\n","                weight_df.loc[i, IC_Large.major_axis[j]] = IR\n","        #weight_df = weight_df.apply(lambda x: x / sum(x))\n","        weight = weight_df\n","        Factors_sum = pd.DataFrame(0, columns=self.data.minor_axis, index=weight.columns)\n","        for i in self.data_2009.items:\n","            df = self.data_2009[i]\n","            new_df = df.mul(weight.loc[i], axis=0)\n","            Factors_sum = Factors_sum + new_df\n","        return Factors_sum\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def Merge_and_store_factors_Dynamic():\n","    Factor_dict = {}\n","    for i in ['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY']:\n","        Factor_data = Large_factor_merge(i).Factors_merge_dynamic()\n","        Factor_dict[i] = Factor_data\n","        file_path = path + \"\\\\H3 Data\\\\Large Factor Dynamic Data\\\\\" + i + \".csv\"\n","        Factor_data.to_csv(file_path)\n","    Large_factor = pd.Panel(Factor_dict)\n","    return Large_factor\n","Large_factor_dynamic = Merge_and_store_factors_Dynamic()\n","# when you want to use one factor,you can edit'Large_factor[the name of the factor]'"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def Merge_and_store_factors_Static():\n","    Factor_dict = {}\n","    for i in ['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY']:\n","        Factor_data = Large_factor_merge(i).Factors_merge_Static()\n","        Factor_dict[i] = Factor_data\n","        file_path = path + \"\\\\H3 Data\\\\Large Factor Static Data\\\\\" + i + \".csv\"\n","        Factor_data.to_csv(file_path)\n","    Large_factor = pd.Panel(Factor_dict)\n","    return Large_factor\n","Large_factor_Static = Merge_and_store_factors_Static()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","def get_Large_Factors(factor_name, type):\n","    category = \"Large Factor \" + type\n","    data = get_data(factor_name, category=category)\n","    return data\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def overview_Large_factors(type):\n","    # Get an overview of data after processing.\n","    plt.figure(figsize = (10, 10))\n","    for i in range(7):\n","        plt.subplot(int(\"33\" + str(i+1)))\n","        factor_name = get_large_factors_list()[i]\n","        sns.distplot(get_values(\n","            data = get_Large_Factors(factor_name,type)\n","        ))\n","        plt.title(factor_name)\n","    plt.suptitle(\"Â§ßÁ±ªÂõ†Â≠êÂú®AËÇ°ÁöÑÂéÜÂè≤Êï∞ÊçÆÂàÜÂ∏É(\" + type + ' synthesis)')\n","    plt.savefig(path + \"\\\\H3 Plots\\\\Large Factors \"+ type +\".png\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÂêàÊàêÂêéÁöÑÂ§ßÁ±ªÂõ†Â≠êÊï∞ÊçÆÂ¶Ç‰∏ãÂõæÔºåÂä®ÊÄÅÊùÉÈáçÂêàÊàêüëá„ÄÇ"],"metadata":{}},{"source":["overview_Large_factors('dynamic')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ÈùôÊÄÅÊùÉÈáçÂêàÊàêüëá„ÄÇ"],"metadata":{}},{"source":["overview_Large_factors('Static')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # STEP 4"],"metadata":{}},{"source":["\n","def get_regression_data(start_year, type):\n","    # get 7+1 data list for one stock\n","    data = pd.DataFrame(\n","        columns=[\n","            'return', 'VALUE', 'GROWTH', 'PROFIT', \n","            'QUALITY', 'MOMENTUM', 'VOLATILITY', 'LIQUIDITY'\n","        ]\n","    )\n","    for factor_name in [\n","        'VALUE', 'GROWTH', 'PROFIT', \n","        'QUALITY', 'MOMENTUM', 'VOLATILITY', 'LIQUIDITY'\n","    ]:\n","        data[factor_name] = get_Large_Factors(factor_name, type).loc[start_year]\n","    \n","    data['return'] = get_data(\"pct_chg_1m\", category=\"Processed\").loc[start_year]\n","\n","    return data\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def regression_model(y, X):\n","    model = sm.WLS(y, X)  # Âä†ÊùÉÊúÄÂ∞è‰∫å‰πòÊ≥ï Ëß£ÂÜ≥ÂºÇÊñπÂ∑ÆÊÄß\n","    results = model.fit()\n","    # results.summary\n","    return results.params\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","\n","def run_regression(type):\n","    time_list = get_Large_Factors('VALUE', type).index[:-1]\n","    param_df = pd.DataFrame(columns=time_list)\n","    for start_year in time_list:\n","        regression_data = get_regression_data(start_year, type)\n","        y = regression_data['return']\n","        X = regression_data.iloc[:, 1:]\n","        param = regression_model(y, X)\n","        param_df[start_year] = param\n","    return param_df\n","\n","# run_regression('Static')==>ËøõË°åÂõûÂΩí\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# ‰º∞ËÆ°Âõ†Â≠êÈ¢ÑÊúüÊî∂ÁõäÔºåÊ≠§Â§ÑÈááÁî®N=12ÁöÑÂéÜÂè≤ÂùáÂÄºÊ≥ï\n","\n","\n","def estimated_factor_expected_income(type):\n","    F = run_regression(type).T\n","    N = 12\n","    time_list = get_Large_Factors('VALUE', type).index\n","    F_predict = pd.DataFrame(columns=F.columns)\n","    for i in range(N, len(time_list)):\n","        F_predict.loc[time_list[i]] = list(F.iloc[i-N:i].mean())\n","    return F_predict\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # STEP 5"],"metadata":{}},{"source":["# ÂÅáËÆæ‰∫Ü‰πãÂâçÂæàÂ§öÂõûÂΩíÁöÑËÆ°ÁÆóÁªìÊûú\n","from scipy.optimize import minimize\n","Factor_income =pd.DataFrame(-1+2*np.random.random((121,9)),columns=['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY','INDUSTRY','SIZE'],\n","                            index = get_data('ps_ttm', category=\"Neutralized\").index)\n","Stock_predict = pd.DataFrame(-0.1+np.random.random((300,1))/3,columns=['yeild_forecast'],index = get_data('ps_ttm', category=\"Neutralized\").columns)\n","Factor_predict = pd.DataFrame(-0.1+np.random.random((300,9))/3,columns=['VALUE','GROWTH','PROFIT','QUALITY','MOMENTUM','VOLATILITY','LIQUIDITY','INDUSTRY','SIZE'],index = get_data('ps_ttm', category=\"Neutralized\").columns)\n","#ÊØèÂè™ËÇ°Á•®ÁöÑÂú®‰∏çÂêåÊó∂Èó¥ÁÇπÁöÑÊÆãÂ∑ÆÔºåÂèØ‰ª•Á≠â‰∫éÂÆûÈôÖÁöÑËÇ°Á•®Êî∂ÁõäÁéá-È¢ÑÊµãÁöÑËÇ°Á•®Êî∂ÁõäÁéá\n","Stock_Residual = pd.DataFrame(-0.1+np.random.random((121,300))/5,columns = get_data('ps_ttm', category=\"Neutralized\").columns,index = get_data('ps_ttm', category=\"Neutralized\").index)\n","\n","\n","class Portfolio_Optimization(object):\n","    def __init__(self, Target_factors ,time_window):\n","        self.Target_factors = Target_factors\n","        self.time_window = time_window\n","\n","    def Factor_covariance(self):\n","        factors = Factor_income.iloc[-self.time_window:]\n","        Cov = np.cov(factors.values.T)\n","        return Cov\n","\n","    # È¢ÑÊµãÊÆãÂ∑ÆÈ£éÈô©Ëøô‰∏ÄÈÉ®ÂàÜÂæàÂ§çÊùÇÔºåÁî®Âà∞ÂçäË°∞ÊúüÊùÉÈáçÂíåË¥ùÂè∂ÊñØÊî∂Áº©ÔºåÂíåÊ≥¢Âä®ÊÄßË∞ÉÊï¥ÔºåÂÖ∂ÂÆû‰∏çÁ∫¶ÊùüÈ£éÈô©Êó∂Ôºå‰∏çÁî®ËÆ°ÁÆóÊ≠§È°πÔºåÂÖàÊääÊ°ÜÊû∂Êê≠Ëµ∑Êù•Ôºå\n","    # ÂêéÈù¢ËÆ°ÁÆóÁªÑÂêàÁöÑÂ§èÊôÆÊØîÁéáË¶ÅÁî®Âà∞ÁªÑÂêàÊñπÂ∑ÆÔºåÂ∞±Ë¶ÅÁî®Âà∞ÊÆãÂ∑ÆÈ£éÈô©ÔºåÂêéÁª≠Á†îÁ©∂Êàë‰ª¨ÂÜç‰ªîÁªÜÁ†îÁ©∂Ëøô‰∏ÄÈÉ®ÂàÜÂÖ∑‰ΩìÊÄé‰πàÁÆó\n","    def Trait_risk_forecast(self):\n","        Res = pd.DataFrame(-0.1+np.random.random((300,300))/5)\n","        return Res\n","\n","    def optimization(self):\n","        Cov = self.Factor_covariance()\n","        Res = self.Trait_risk_forecast()\n","        yeild_T_1 = Stock_predict\n","\n","        #ÈùûÁ∫øÊÄßËßÑÂàí\n","        x0 = np.random.rand(300)\n","        x0 /= sum(x0)\n","        Non_target_factors = list(set(Large_Factors_list) ^ set(self.Target_factors))\n","        n = len(Non_target_factors)\n","        m = list(range(n))\n","        b = [0]*9\n","        b[0:n-1] = m\n","        # Ê≠§Â§ÑÊàëÂ∞ùËØï‰∫ÜÂêÑÁßçÊñπÊ≥ïÔºåÁî®iÈÅçÂéÜÈùûÁõÆÊ†áÁ∫ØÂõ†Â≠êÁÑ∂ÂêéÁîüÊàêÊù°‰ª∂Ôºå‰ΩÜÊòØÁîüÊàêÁöÑÊù°‰ª∂Âú®‰ºòÂåñÊ®°Âûã‰∏≠Ê≤°ÊúâË¢´ÊàêÂäüÁ∫¶ÊùüÔºåÊúÄÂêéÂè™ËÉΩÂÖ®ÈÉ®ÂÜôÂá∫Êù•9‰∏™Âõ†Â≠êÊù°‰ª∂Ôºå\n","        # ÂØπ‰∫éÁõÆÊ†áÁ∫ØÂõ†Â≠êÔºåÂ∫èÊï∞ÂèñÁöÑ0ÔºåÂç≥Êù°‰ª∂ÊòØÈáçÂ§çÁöÑÈùûÁõÆÊ†áÂõ†Â≠êÁ∫¶ÊùüÔºåËøôÈáåÁöÑÊù°‰ª∂Êï∞ÊçÆÁ±ªÂûãÊòØtupleÔºåtuple‰∏çËÉΩË¢´Â¢ûÂä†ÔºåÊàëËØïËøáÂÖàÁî®listÊ∑ªÂä†ÁÑ∂ÂêéËΩ¨‰∏∫tuple,\n","        # ‰ªçÁÑ∂Ê≤°ÊúâË¢´ÊàêÂäüËØÜÂà´ÔºåÂ¶ÇÊûúÊúâÊõ¥ÁÆÄ‰æøÁöÑÊñπÊ≥ïÔºåÊ¨¢ËøéÊèêÂá∫\n","        # ÊúÄÂ∞èÂåñÁöÑÂáΩÊï∞\n","        func = lambda x: -(yeild_T_1 * np.mat(x).T).sum()[0]\n","        cons4 = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},\n","                 {'type': 'ineq','fun': lambda x:(0.03-abs((Factor_predict[[Non_target_factors[b[0]]]]*np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[1]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[2]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[3]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq', 'fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[4]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[5]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[6]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[7]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[8]]]] * np.mat(x).T).sum()[0]))},\n","                 )\n","        # Â¶ÇÊûúË¶ÅÊ∑ªÂä†Ê≥¢Âä®ÊÄßÁ∫¶ÊùüÔºåÊù°‰ª∂Ë¶ÅÊîπ‰∏∫‰ª•‰∏ãÔºåÊàëÂÜôÁöÑÊòØÈôêÂà∂Ê≥¢Âä®Â∞è‰∫é3%\n","        '''cons4 = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},\n","                 {'type': 'ineq','fun': lambda x: (0.03 -((np.mat(Factor_predict).T*np.mat(x).T).T*Cov*(np.mat(Factor_predict).T*np.mat(x).T))[0,0]\n","                                    +(np.mat(x)*np.mat(Res)*np.mat(x).T))[0,0]},\n","                {'type': 'ineq', 'fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[0]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[1]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[2]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[3]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[4]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[5]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[6]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[7]]]] * np.mat(x).T).sum()[0]))},\n","                 {'type': 'ineq','fun': lambda x: (0.03 - abs((Factor_predict[[Non_target_factors[b[8]]]] * np.mat(x).T).sum()[0]))},\n","                 )'''\n","        c = (0,1)\n","        bnds = tuple([c]*300)#ËæπÁïåÊù°‰ª∂‰∏∫0-1\n","        res = minimize(func, x0, method='SLSQP', constraints=cons4,bounds = bnds)\n","        Stock_weight = pd.DataFrame(res.x,columns=['Portfolio Weight'],index = Stock_predict.index)\n","        return  Stock_weight, -res.fun\n","\n","# ÁõÆÊ†áÁ∫ØÂõ†Â≠ê‰∏∫'VALUE','GROWTH','PROFIT'Ôºå‰ΩøÁî®ÂéÜÂè≤Êó∂Èó¥ÊÆµ‰∏∫ËøáÂéª32‰∏™ÊúàÔºå‰ªÖÂØπÈùûÁõÆÊ†áÁ∫ØÂõ†Â≠êÂÅèÁ¶ªÂÅöÁ∫¶ÊùüÊù°‰ª∂ÔºåÊúÄÂ§ßÂåñÊî∂ÁõäÔºåËøîÂõûÊùÉÈáçÂíåÁªÑÂêàÊî∂Áõä\n","#[Stock_weight,Portfolio_Return]= Portfolio_Optimization(['VALUE','GROWTH','PROFIT'],32).optimization()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}